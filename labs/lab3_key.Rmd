---
title: "Lab 3"
author: "Key"
date: "4/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rio)
library(tidymodels)
```

Read in the `state-test-simulated` data.

```{r, data}
math <- import("C:/Users/jnese/Desktop/BRT/Teaching/Predictive-Modeling/c4-ml-2020/c4-ml-2020/data/state-test-simulated.csv") %>% 
  as_tibble()

math <- math %>% 
  drop_na(lat) 
```


1. Initial Split

Split the data into a training set and a testing set as two named objects. 

```{r, initial_split}
set.seed(3000)

math_split <- initial_split(math) 

math_train <- training(math_split)
math_test  <- testing(math_split)
```

2. Resample

Use 10-fold cross-validation to resample the traning data, stratifying by `srt_tst_typ`.

```{r, resample}

set.seed(3000)
cv_folds <- vfold_cv(math_train, strata = "srt_tst_typ")
```

3. Preprocess

Create a recipe object that includes:
* a formula model with `score` predicted by 4 predictors
* center and scale all numeric predictors
* dummy code all nominal predictors

```{r, preprocess}

lasso4_rec <- 
  recipe(
    score ~ enrl_grd + srt_tst_typ + lat + lon, 
    data = math_train
  ) %>%
  step_dummy(all_nominal()) %>%
  step_normalize(all_numeric()) 
```

4. Parsnip model

Create a lasso `{parsnip}` model where the penalty hyperparameter is set to be tuned.

```{r, lasso}

mod_lasso <- linear_reg() %>%
  set_engine("glmnet") %>% 
  set_args(penalty = tune(),
           mixture = 1)

```

7. Fit a tuned lasso model

Complete the code maze below to fit a tuned lasso model.

```{r, lasso_fit_1}

lasso_grid <- grid_regular(penalty())

# lasso_fit_1 <- tune_grid(
#   ____, 
#   preprocessor = ____,
#   resamples = ____,
#   grid = lasso_grid,
#   control = tune::control_resamples(verbose = TRUE,
#                                     save_pred = TRUE)
# )

lasso_fit_1 <- tune_grid(
  mod_lasso, 
  preprocessor = lasso4_rec,
  resamples = cv_folds,
  grid = lasso_grid,
  control = tune::control_resamples(verbose = TRUE,
                                    save_pred = TRUE)
)
```

* Question A
  + How many models were fit to each fold of `lasso_fit_1`? (Please provide a numeric answer, *and* use code to corroborate your answer.)
```{r}
# 3 models for each fold

lasso_fit_1 %>% 
  collect_metrics() %>% 
  filter(`.metric` == "rmse") %>% 
  nrow()
# OR
lasso_fit_1$.metrics[[1]]
```

  + Use code to list the different values of `penalty()` that were used.

```{r}

lasso_fit_1 %>% 
  collect_metrics() %>% 
  filter(`.metric` == "rmse") %>% 
  select(penalty)
```

8. Fit another tuned lasso model

Use your code from (7) above to complete the code maze below to fit a second tuned lasso model.

```{r, lasso_fit_2}

# lasso_fit_2 <- tune_grid(
#   ____, 
#   preprocessor = ____,
#   resamples = ____,
#   metrics = yardstick::metric_set(rmse),
#   control = tune::control_resamples(verbose = TRUE,
#                                     save_pred = TRUE)
# )

lasso_fit_2 <- tune_grid(
  mod_lasso,
  preprocessor = lasso4_rec,
  resamples = cv_folds,
  control = tune::control_resamples(verbose = TRUE,
                                    save_pred = TRUE)
)
```

* Question B

  + How many models were fit to each fold of `lasso_fit_2`? (Please provide a numeric answer, *and* use code to corroborate your answer.)

```{r}
# 10 models for each fold

lasso_fit_2 %>% 
  collect_metrics() %>% 
  filter(`.metric` == "rmse") %>% 
  nrow()
# OR
lasso_fit_2$.metrics[[1]] %>% 
  filter(`.metric` == "rmse")
```

  + If this is different than the number of models of `lasso_fit_1`, please explain why.
**This is different because the default for** `grid_regular(penalty())` **has** `levels = 3`. **See** `?grid_regular()`. For *lasso_fit_2*, we used the default for `fit_resamples` which sets `grid = 10`.
  
  + Use code to list the different values of `penalty()` that were used for *lasso_fit_2*.

```{r}
lasso_fit_2 %>% 
  collect_metrics() %>% 
  filter(`.metric` == "rmse") %>% 
  select(penalty)
```

9. Complete the necessary steps to create and fit a tuned lasso model that has at least seven predictors (use any tuning grid you like).

```{r, lasso8}

lasso_rec8 <- 
  recipe(
    score ~ enrl_grd + srt_tst_typ + lat + lon + sp_ed_fg + econ_dsvntg + gndr, 
    data = math_train
  ) %>%
  step_naomit(everything()) %>%
  step_dummy(all_nominal()) %>%
  step_normalize(all_numeric()) %>%
  prep() %>% 
  juice() %>% View()

lasso_fit_8 <- tune_grid(
  mod_lasso,
  preprocessor = lasso_rec8,
  resamples = cv_folds,
  control = tune::control_resamples(verbose = TRUE,
                                    save_pred = TRUE)
)

```

10. Compare the metrics from the lasso model with 4 predictors to the lasso model with 7+ predicors. Which is best?

```{r}

```

11. Fit a tuned elastic net model with the same predictors from (9). 
  + Create a new `{parsnip}` model
  + Use the same recipe from (9) above
  + Create and apply a regular grid for the elastic net model
  + Compare the metrics from the elastic net model to the lasso model from (9). Which would you choose for your final model? What are the best hyperparameters for that model?



