---
title: "Variable selection and model performance"
subtitle: "A walkthrough"
author: "Daniel Anderson "
date: "Week 3, Class 1"
output:
  xaringan::moon_reader:
    css: ["default", "uo", "uo-fonts", "hygge", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-dune-light
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.width = 13, 
                      message = FALSE, 
                      warning = FALSE,
                      echo = TRUE)
library(tidyverse)
theme_set(theme_minimal(25))
```

# Agenda

* Create a stratified split dataset

* Specify a linear regression model

* Use $k$-fold evaluation to evaluate different model fits 

* Compare models based on evaluation criteria

* Visualize and inspect measures of variable importance to refine model

* Evaluate on test set

---
# Scenario
* Imagine you have a dataset with a lot of variables

* You don't know much about what relates to the outcomes


--
* .bolder[.b[GOAL]]: Figure out the most important subset of variables that relate to the outcome


---
# Workflow
* Split the dataset into train/test sets


--
* Prepare the dataset for $k$-fold cross validation


--
* Determine the type of model you will fit, and the problem type (regression or classification)


--
* Compare different models against the chosen objective funtion through $k$-fold CV


--
* Settle on a final model, and evaluate it against the test dataset


---
# Load the data
Please follow along!

--
### Dealing with file size
A few people had issues during the last lab because the file is rather large.

Let's use a new package that uses "lazy loading". We can help it further by providing a column specification.

---
```{r col-spec}
library(tidyverse)

col_specs <- cols(
  .default = col_character(),
  id = col_double(),
  attnd_dist_inst_id = col_double(),
  attnd_schl_inst_id = col_double(),
  enrl_grd = col_double(),
  calc_admn_cd = col_logical(),
  partic_dist_inst_id = col_double(),
  partic_schl_inst_id = col_double(),
  lang_cd = col_character(), #<<
  score = col_double(),
  classification = col_double(),
  ncessch = col_double(),
  lat = col_double(),
  lon = col_double()
)
```

---
# How did I get this?
You could do it manually, but that would be a lot of work. Instead, just read in, say, 10 rows, and make any adjustments to the specs you need to after that.

```{r readr-fast, message = TRUE}
read_csv(here::here("data", "edld-654-spring-2020", "train.csv"),
         n_max = 10)
```


---
```{r load-data}
d <- vroom::vroom(
  here::here("data", "edld-654-spring-2020", "train.csv"),
  col_types = col_specs
)
```

---
class: inverse center middle
# House cleaning

---
# Initial cleanup
* Remove classification column
* Recode missing data on a few variables

```{r initial-prep}
set.seed(123)
d <- d %>% 
  select(-classification)  %>% 
  mutate(lang_cd = ifelse(is.na(lang_cd), "E", lang_cd),
         ayp_lep = ifelse(is.na(ayp_lep), "G", ayp_lep),
         tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt))) %>% 
  # only use a sample
  group_by(lang_cd) %>% 
  sample_frac(.25) %>% 
  ungroup() 
```

---
# Check out the data

```{r inspect-d}
d %>% 
  mutate_if(is.character, as.factor) %>% 
  map(summary)
```

---
# Remove constant variables
* Also make things really easy and remove missing data

```{r rm-constants}
d <- d %>% 
  # remove any rows or columns that are fully missing
  janitor::remove_empty(c("rows", "cols")) %>% 
  
  # drop any row that has a missing value
  drop_na() %>% 
  
  # drop any column that doesn't have more than 1 unique value
  select_if(~length(unique(.x)) > 1)
```


---
class:inverse center middle
# Split the data

---
# Split the data
Note that this is already our "training" data, but we have to treat it like our full data (we don't have labels for the "test" data).

--

* Set a seed


--
* Create an initial split
  + Consider any stratification variables
  + Consdider what proportion you want in test/training


--
* Pull the training data
  + You could pull the test data too, but I usually wait until I get to the evaluation period.

--
```{r initial-split}
library(tidymodels)
splt <- initial_split(d, strata = lang_cd)
train <- training(splt)
```

---
# Check our stratification

.pull-left[
```{r check-strat1}
d %>% 
  count(lang_cd) %>% 
  mutate(prop = n/sum(n))
```
]

.pull-right[
```{r check-strat2}
train %>% 
  count(lang_cd) %>% 
  mutate(prop = n/sum(n))
```
]

---
# Create $k$-folds

* For now, we'll just use the default 10 splits - note this may not *always* work best


--
* High variance betweeen folds? Consider repeated $k$-fold CV. (not the same thing as increasing number of folds)


--
* How big is your sample? Really large - probably don't need to worry. If small, may need to reduce number of folds


--
* Remember to declare `strata` if needed


--

```{r cv}
cv <- vfold_cv(train, strata = lang_cd)
```

---
# Check out the structure
* Remember, just a list column.
```{r print-cv}
cv
```

---
# Looking into it more
* Use `analysis` to extract the data from the corresponding fold that you will use to estimate the model

* Use `assessment` to extract all .ital[other] data from the corresponding fold, which you will use to evaluate the model 


--

.pull-left[

```{r prop1}
cv$splits[[1]] %>% 
  assessment() %>% #<<
  nrow()

3482 / (3482 + 31336)
```

]

.pull-right[

```{r prop2}
cv$splits[[1]] %>% 
  analysis() %>% #<<
  nrow()

31336 / (3482 + 31336)
```

]


---
# Double check our stratification

```{r strata-fold-check}
cv %>% 
  mutate(assessment_props = map(
    splits, ~
      assessment(.x) %>% 
      count(lang_cd) %>% 
      mutate(prop = n/sum(n))
      )
    ) %>% 
  unnest(assessment_props)
```


---
class: inverse center middle
# Specify a model

---
# Select a model
* As we go further in the term, we'll discuss quite a few different modeling options

--

* For now, we'll just use linear regression

```{r set_model}
mod <- linear_reg()
```

--

* Importantly, we're just setting up the framework here. We're not estimating anything yet.

--

```{r print-model}
mod
```


--
Let's briefly look at the [full set of models](https://tidymodels.github.io/parsnip/articles/articles/Models.html) that can be estimated within this framework.

---
# Specify an "engine"
* You can estimate a linear regression model a few different ways. For example, you could use OLS or Bayes (via `lm` or `rstanarm::stan_lm` (among others)).

* Again, this will matter more when you get to more adavanced models. Let's just use `lm` as the engine for now.

--

```{r set_engine}
mod <- mod %>% 
  set_engine("lm")
mod
```

Full options include: `"lm"`, `"glmnet"`, `"stan"`, `"spark"`, and `"keras"`

---
# Specify the mode

* Is this a regression or classification problem?


--
In this case, regression, so

```{r set_mode}
mod <- mod %>% 
  set_mode("regression")
mod
```

For classification problems, just change `"regression"` to `"classification"`.


--
.b[.bolder[NOTE]]: In this case, we can't estimate a classification problem using linear regression, so it had already set this for us, but it's a good habit to get into anyway.

---
class:inverse center middle
# Fit a model

---
# Fit a model
* Let's start out with fitting a model with gender and special education status, e.g., `score ~ gndr + sp_ed_fg`.


--
* We'll fit this model to the `analysis` portion of every fold, then evaluate it against the `assessment` data in each fold.


--
* We'll do this with the `fit_resamples` function, which takes the following form: `fit_resamples(model, formula, resamples))`


--
* .b[.bolder[NOTE]]: `formula` can also be a `{recipes}` preprocessor object, which we'll talk about later.


--
* There are additional arguments you can pass, but these are the required arguments.

---
# Setting up a recipe
* Before fitting the model, we first have to specify a .ital[.b[recipe]] that tells our engine what model to fit .r[and] and pre-processing steps.


--
* Alternative to `model.matrix`

--
* We will talk about this quite a bit more in two weeks (feature engineering)

* For now, let's just give it the model formula

--

```{r rec1}
baseline_rec <- recipe(score ~ gndr + sp_ed_fg, train)
```

* Note that I provided the recipe the full training dataset as the data source. The recipe is just using the data source to get the column names.

---
# Inspecting the recipe
```{r print-rec1}
baseline_rec
```

* Similar to specifying the model, nothing is actually happening here - it will be applied to each fold.


--
* Define the role of each column in the dataset (you can define ID variables too)


--
* Specify any pre-processing steps (e.g., center/scale)

---
# Fit to resampled data

```{r fit_resamples1}
m1 <- mod %>% 
  fit_resamples(preprocessor = baseline_rec,
                resamples = cv)
m1

```

---
# Check out evaluation metrics

```{r eval-metrics1}
m1_metrics <- m1 %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  pivot_wider(names_from = .metric, 
              values_from = .estimate)

m1_metrics
```

.footnote[Note that the model results were not actually saved]

---
# Fit a second model
### Include interaction

* Recipe gets a bit more complicated here. 

```{r interaction_rec}
interaction_rec <- recipe(score ~ gndr + sp_ed_fg, train) %>% 
  step_dummy(all_predictors()) %>% 
  step_interact(terms = ~ starts_with("gndr"):starts_with("sp_ed_fg"))
interaction_rec
```

---
# Fit the model

```{r fit-m2-resamples}
m2 <- mod %>% 
  fit_resamples(interaction_rec, cv)
m2
```


---
# Check evaluation metrics again

```{r eval-metrics2}
m2_metrics <- m2 %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  pivot_wider(names_from = .metric, 
              values_from = .estimate)

m2_metrics
```


---
# Join metrics

```{r join-metrics1}
metrics_joined <- left_join(m1_metrics, m2_metrics, 
          by = c("id", ".estimator"),
          suffix = c("_m1", "_m2"))

metrics_joined
```

---
# Interaction worth it?
### Move back to long
```{r inspect-interaction}
metrics <- metrics_joined %>% 
  pivot_longer(cols = contains("_"),
               names_to = c("metric", "model"),
               values_to = "estimate",
               names_sep = "_")
metrics
```

---
# Inspect by RMSE

```{r rmse-min1}
metrics %>% 
  filter(metric == "rmse") %>% 
  group_by(id) %>% 
  summarize(estimate = min(estimate)) 
```

---

```{r rmse-min2}
metrics %>% 
  filter(metric == "rmse") %>% 
  group_by(id) %>% 
  summarize(estimate = min(estimate)) %>% 
  semi_join(metrics, .) #<<
```


---
```{r rmse-min3}
metrics %>% 
  filter(metric == "rmse") %>% 
  group_by(id) %>% 
  summarize(estimate = min(estimate)) %>% 
  semi_join(metrics, .) %>% 
  count(model) #<<
```


--
### Answer: ðŸ¤·


---
# Alternative way to think about it

```{r change-rmse}
metrics_joined %>% 
  mutate(rmse_diff = round(rmse_m1 - rmse_m2, 3),
         rsq_diff  = round(rsq_m1 - rsq_m2, 3)) %>% 
  select(id, ends_with("diff"))
```

--
### Conclusion: Probs not

---
# Variable importance

* Another way to think about this is how .b[important] specific variables, interactions, or other features of the model are to the estimated predictive accuracy


--

.center[
![](https://koalaverse.github.io/vip/reference/figures/logo-vip.png)
]

---
# VIP
* `vip::vip`: Create a `v`ariable `i`mportance `p`lot
* `vip::vi`: Estimate `v`ariable `i`mportance (numbers)

--
```{r vi1}
library(vip)
m <- lm(score ~ gndr + sp_ed_fg + gndr:sp_ed_fg, data = train)
vi(m)
```

---
# Graphically
```{r vip1}
vip(m)
```

---
# How to improve this model
* I've approached this from a (light) theoretical variable-inclusion perspective

  + Hopefully you have domain knowledge in your application that allows you to start from a meaningful place.
  
  + Other variables that theoretically relate that we could start with (e.g., grade level, economic disadvantage status)
 

--
* What if I didn't know? We could start out by looking at all variables, and choosing the most important


--

```{r vip2}
vip_1a <- vip(lm(score ~ ., train))
```

---
```{r show-vip2, fig.height = 6.5}
vip_1a
```


--

This basically tells us that outside of sped (which we've included already) gifted/talented (tag), economic disadvantage, test date, and ethnic code are important predictors. Let's look a bit deeper.

---
# Numerically
```{r vip2b1}
vip_1b <- vi(lm(score ~ ., train))
vip_1b
```


---
# Interactions
You can also use vip with interactions

```{r vip2a, cache = TRUE, fig.height = 5.75}
vip_2a <- vip(lm(score ~ .^2, train))
vip_2a
```

---
# New model

```{r fit_resamples3}
post_vip_rec <- recipe(score ~ tag_ed_fg + sp_ed_fg + econ_dsvntg +
                               tst_dt + ethnic_cd, 
                       data = train)

m3 <- mod %>% 
  fit_resamples(post_vip_rec, cv)

m3
```


---
# M3 metrics

```{r m3-metrics}
m3 %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  pivot_wider(names_from = .metric, 
              values_from = .estimate)
```

---
## Improving model performance further
* Other variables? (e.g., grade level)

* Interactions?
  + `step_interact`

* Nonlinearity?
  + `step_poly`


--
### Refining the model
* We have quite a bit of repeated code here. If this were the strategy we were using, we'd probably want to write functions for the model comparisons.


---
# How are we doing on time?
### Skip if not enough time, otherwise...

Try to refine this model further to boost model performance more! Consider modifications listed in previous slide (inclusion of other variables, nonlinear terms, or interactions)

`r countdown::countdown(minutes = 10, top = 0.5, bottom = 0.5, right = 0.5, left = 0.75)`

---
# Evaluation on test set
* Note we wouldn't usually jump to this point yet, but let's try.

--
### First: Fit model to full training data

```{r fit-full-training}
test_mod <- mod %>% 
  fit(score ~ tag_ed_fg + sp_ed_fg + econ_dsvntg +
              tst_dt + ethnic_cd, 
      data = train) 
```

---
### Next: Extract test dataset

```{r test}
test <- testing(splt)
test
```

---
# Make predictions

```{r preds}
preds <- test %>% 
  mutate(estimate = predict(test_mod$fit, newdata = .)) %>% 
  select(truth = score, estimate)
preds
```

---
# Evaluate prediction

```{r test-eval}
bind_rows(
  rmse(preds, truth, estimate),
  rsq(preds, truth, estimate)
)
```

---
class: inverse center middle

# Evaluation metrics
### Quickly

---
# Big picture
* We've spent most of today talking about the process of building a linear model. But how you determine what is "best" depends (in part) on your evaluation metric.

--

.Large[.center[For regression problems]]

--

.pull-left[
### Well known metrics

* MSE - mean square error
* RMSE - root mean square error
* $R^2$
]

--

.pull-right[
### Less well-known metrics
* MAE - mean absolute error
* RPIQ - ratio of performance to inter-quartile
* Huber loss - balances MSE and MAE
]

--

see [yardstick documentation](https://tidymodels.github.io/yardstick/reference/index.html) for complete reference of metrics implemented.

---
# A few notes

* RMSE is the mean error, but on the scale of the outcome
  + .b[Pros]: Generally fairly interpretable - how much are your predictions off by, on average?
  + .r[Cons]: The mean (generally) can be susceptible to outliers


--
* MAE is the median error
  + .b[Pros]: Less influenced by outliers
  + .r[Cons]: Somewhat less familiar; sometimes the outlier cases are highly important


--
* Huber Loss balances (R)MSE and MAE
  + Quadratic for small residual values, linear for large residual values
  + .b[Pros]: Sensitivity of RMSE with robustness of MAE
  + .r[Cons]: Less familiar and interpretable; does not translate to common measure of central tenancy

---
# Huber loss 
Compared to .b[squared errors]

.center[

```{r huber-loss, out.height = "400px", echo = FALSE}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Huber_loss.svg/1920px-Huber_loss.svg.png")
```

]

---
# Overall takeaway
* There are more metrics than described above.

--
* Choosing a metric can be really important, and often is driven by the specific problem

.g[(Example from handwriting recognition algorithm)]


--
* We haven't talked about classification yet, but there are similarly many different evaluation metrics there


--
* We'll try to incorporate a discussion of these as we go along

---
# Kaggle
### Moving back to our model
* What do we need to do to make predictions from this model for kaggle?


--
* Preprocess the `train.csv` using the recipe we used above (particularly if you have interactions, polynomial terms, etc.)


--
* Fit the model to the preprocessed data


--
* Make predictions
  
  + Will we be able to evaluate?
  

--
* Upload a csv file that has a `Id` and `Predicted` columns. 


---
class: inverse center middle

# Next time
### Lab 2
