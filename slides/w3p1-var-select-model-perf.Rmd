---
title: "Variable selection and model performance"
subtitle: "A walkthrough"
author: "Daniel Anderson "
date: "Week 3, Class 1"
output:
  xaringan::moon_reader:
    css: ["default", "uo", "uo-fonts", "hygge", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-dune-light
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.width = 13, 
                      message = FALSE, 
                      warning = FALSE,
                      echo = TRUE)
library(tidyverse)
theme_set(theme_minimal(25))
```

# Agenda

* Create a stratified split dataset

* Specify a linear regression model

* Use $k$-fold evaluation to evaluate different model fits 

* Compare models based on evaluation criteria

* Visualize and inspect measures of variable importance to refine model

* Evaluate on test set

---
# Scenario
* Imagine you have a dataset with a lot of variables

* You don't know much about what relates to the outcomes


--
* .bolder[.b[GOAL]]: Figure out the most important subset of variables that relate to the outcome


---
# Workflow
* Split the dataset into train/test sets


--
* Prepare the dataset for $k$-fold cross validation


--
* Determine the type of model you will fit, and the problem type (regression or classification)


--
* Compare different models against the chosen objective funtion through $k$-fold CV


--
* Settle on a final model, and evaluate it against the test dataset


---
# Load the data
* Please follow along!

```{r load-data}
library(tidyverse)
library(tidymodels)
d <- read_csv(here::here("data", "edld-654-spring-2020", "train.csv"))
head(d)
```

---
class: inverse center middle
# House cleaning

---
# Initial cleanup
* Remove classification column
* Recode missing data on a few variables

```{r label, options}
d <- d %>% 
  select(-classification)  %>% 
  mutate(lang_cd = ifelse(is.na(lang_cd), "E", lang_cd),
         calc_admn_cd = ifelse(is.na(calc_admn_cd), "G", calc_admn_cd),
         ayp_lep = ifelse(is.na(ayp_lep), "G", ayp_lep),
         tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt)))
```

* Let's also make things really easy on ourselves, and remove all missing data.

```{r rem-missing}
d <- d %>% 
  mutate_if(is.character, as.factor) %>% 
  drop_na()
```

---
# Check out the data

```{r inspect-d}
d %>% 
  mutate_if(is.character, as.factor) %>% 
  map(summary)
```

---
# Remove constant variables

```{r rm-constants}
d <- d %>% 
  select(-grp_rpt_dist_partic, -rc_dist_partic,
         -ayp_dist_partic, -calc_admn_cd)
```

---
class:inverse center middle
# Split the data

---
# Split the data
Note that this is already our "training" data, but we have to treat it like our full data (we don't have labels for the "test" data).

--

* Set a seed


--
* Create an initial split
  + Consider any stratification variables
  + Consdider what proportion you want in test/training


--
* Pull the training data
  + You could pull the test data too, but I usually wait until I get to the evaluation period.

--
```{r initial-split}
set.seed(123)
splt <- initial_split(d, strata = lang_cd)
train <- training(splt)

```

---
# Check our stratification

.pull-left[
```{r check-strat1}
d %>% 
  count(lang_cd) %>% 
  mutate(prop = n/sum(n))
```
]

.pull-right[
```{r check-strat2}
train %>% 
  count(lang_cd) %>% 
  mutate(prop = n/sum(n))
```
]

---
# Create $k$-folds

* For now, we'll just use the default 10 splits - note this may not *always* work best


--
* High variance betweeen folds? Consider repeated $k$-fold CV. (not the same thing as increasing number of folds)


--
* How big is your sample? Really large - probably don't need to worry. If small, may need to reduce number of folds


--
* Remember to declare `strata` if needed


--

```{r cv}
cv <- vfold_cv(train, strata = lang_cd)
```

---
# Check out the structure
* Remember, just a list column.
```{r print-cv}
cv
```

---
# Looking into it more
* Use `analysis` to extract the data from the corresponding left out fold, or `assessment` to extract all the data *except* the left out fold 


--

.pull-left[

```{r prop1}
cv$splits[[1]] %>% 
  assessment() %>% #<<
  nrow()

13921 / (13921 + 125281)
```

]

.pull-right[

```{r prop2}
cv$splits[[1]] %>% 
  analysis() %>% #<<
  nrow()

125281 / (13921 + 125281)
```

]


---
# Double check our stratification

```{r strata-fold-check}
cv %>% 
  mutate(assessment_props = map(
    splits, ~
      assessment(.x) %>% 
      count(lang_cd) %>% 
      mutate(prop = n/sum(n))
      )
    ) %>% 
  unnest(assessment_props)
```


---
class: inverse center middle
# Specify a model

---
# Select a model
* As we go further in the term, we'll discuss quite a few different modeling options

--

* For now, we'll just use linear regression

```{r set_model}
mod <- linear_reg()
```

--

* Importantly, we're just setting up the framework here. We're not estimating anything yet.

--

```{r print-model}
mod
```

---
# Specify an "engine"
* You can estimate a linear regression model a few different ways. For example, you could use OLS or Bayes (via `lm` or `rstanarm::stan_lm` (among others)).

* Again, this will matter more when you get to more adavanced models. Let's just use `lm` as the engine for now.

--

```{r set_engine}
mod <- mod %>% 
  set_engine("lm")
mod
```

Full options include: `"lm"`, `"glmnet"`, `"stan"`, `"spark"`, and `"keras"`

---
# Specify the mode

* Is this a regression or classification problem?


--
In this case, regression, so

```{r set_mode}
mod <- mod %>% 
  set_mode("regression")
mod
```

For classification problems, just change `"regression"` to `"classification"`.


--
.b[.bolder[NOTE]]: In this case, we can't estimate a classification problem using linear regression, so it had already set this for us, but it's a good habit to get into anyway.

---
class:inverse center middle
# Fit a model

---
# Fit a model
* Let's start out with fitting a model with gender and special education status, e.g., `score ~ gndr + sp_ed_fg`.


--
* We'll fit this model to the `analysis` portion of every fold, then evaluate it against the `assessment` data in each fold.


--
* We'll do this with the `fit_resamples` function, which takes the following form: `fit_resamples(model, formula, resamples))`


--
* .b[.bolder[NOTE]]: `formula` can also be a `{recipes}` preprocessor object, which we'll talk about later.


--
* There are additional arguments you can pass, but these are the required arguments.

---
# Setting up a recipe
* Before fitting the model, we first have to specify a .ital[.b[recipe]] that tells our engine what model to fit .r[and] and pre-processing steps.


--
* Alternative to `model.matrix`

--
* We will talk about this quite a bit more in two weeks (feature engineering)

* For now, let's just give it the model formula

--

```{r rec1}
baseline_rec <- recipe(score ~ gndr + sp_ed_fg, train)
```

* Note that I provided the recipe the full training dataset as the data source. The recipe is just using the data source to get the column names.

---
# Inspecting the recipe
```{r print-rec1}
baseline_rec
```

* Similar to specifying the model, nothing is actually happening here - it will be applied to each fold.


--
* Define the role of each column in the dataset (you can define ID variables too)


--
* Specify any pre-processing steps (e.g., center/scale)

---
# Fit to resampled data

```{r fit_resamples1}
m1 <- mod %>% 
  fit_resamples(preprocessor = baseline_rec,
                resamples = cv)
m1

```

---
# Check out evaluation metrics

```{r eval-metrics1}
m1_metrics <- m1 %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  spread(.metric, .estimate)

m1_metrics
```

.footnote[Note that the model results were not actually saved]

---
# Fit a second model
### Include interaction

* Recipe gets a bit more complicated here. 

```{r interaction_rec}
interaction_rec <- recipe(score ~ gndr + sp_ed_fg, train) %>% 
  step_dummy(all_predictors()) %>% 
  step_interact(terms = ~ starts_with("gndr"):starts_with("sp_ed_fg"))
interaction_rec
```

---
# Fit the model

```{r fit-m2-resamples}
m2 <- mod %>% 
  fit_resamples(interaction_rec, cv)
m2
```


---
# Check evaluation metrics again

```{r eval-metrics2}
m2_metrics <- m2 %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  spread(.metric, .estimate)

m2_metrics
```


---
# Join metrics

```{r join-metrics1}
metrics_joined <- left_join(m1_metrics, m2_metrics, 
          by = c("id", ".estimator"),
          suffix = c("_m1", "_m2"))

metrics_joined
```

---
# Interaction worth it?
### Move back to long
```{r inspect-interaction}
metrics <- metrics_joined %>% 
  pivot_longer(cols = contains("_"),
               names_to = c("metric", "model"),
               values_to = "estimate",
               names_sep = "_")
metrics
```

---
# Inspect by RMSE

```{r rmse-min1}
metrics %>% 
  filter(metric == "rmse") %>% 
  group_by(id) %>% 
  summarize(estimate = min(estimate)) 
```

---

```{r rmse-min2}
metrics %>% 
  filter(metric == "rmse") %>% 
  group_by(id) %>% 
  summarize(estimate = min(estimate)) %>% 
  semi_join(metrics, .) #<<
```


---
```{r rmse-min3}
metrics %>% 
  filter(metric == "rmse") %>% 
  group_by(id) %>% 
  summarize(estimate = min(estimate)) %>% 
  semi_join(metrics, .) %>% 
  count(model) #<<
```


--
### Answer: ðŸ¤·


---
# Alternative way to think about it

```{r change-rmse}
metrics_joined %>% 
  mutate(rmse_diff = round(rmse_m1 - rmse_m2, 3),
         rsq_diff  = round(rsq_m1 - rsq_m2, 3)) %>% 
  select(id, ends_with("diff"))
```

--
### Conclusion: Probs not

---
# How to improve this model
* I've approached this from a (light) theoretical variable-inclusion perspective

  + Hopefully you have domain knowledge in your application that allows you to start from a meaningful place.
  
  + Other variables that theoretically relate that we could start with (e.g., grade level, economic disadvantage status)
 

--
* What if I didn't know? We could start out by looking at all variables, and choosing the most important


--

```{r vip1}
library(vip)
vip_1a <- vip(lm(score ~ ., train))
```

---
```{r show-vip1}
vip_1a
```


--

This basically tells us that outside of sped (which we've included already) gifted/talented (tag), economic disadvantage, test date, and ethnic code are important predictors. Let's look a bit deeper.

---
```{r vip1b1}
vip_1b <- vi(lm(score ~ ., train))
vip_1b
```

Numbers view of the previous plot

---
# Interactions
You can also use vip with interactions

```{r vip2a, cache = TRUE, fig.height = 5}
vip_2a <- vip(lm(score ~ .^2, train))
vip_2a
```

---
# New model

```{r fit_resamples3}
post_vip_rec <- recipe(score ~ tag_ed_fg + sp_ed_fg + econ_dsvntg +
                               tst_dt + ethnic_cd, 
                       data = train)

m3 <- mod %>% 
  fit_resamples(post_vip_rec, cv)

m3
```


---
# M3 metrics

```{r m3-metrics}
m3 %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  spread(.metric, .estimate)
```

---
## Improving model performance further
* Other variables? (e.g., grade level of general assessment)

* Interactions?
  + `step_interact`

* Nonlinearity?
  + `step_poly`


--
### Refining the model
* We have quite a bit of repeated code here. If this were the strategy we were using, we'd probably want to write functions for the model comparisons.


---
# How are we doing on time?
### Skip if not enough time, otherwise...

Try to refine this model further to boost model performance more! Consider modifications listed in previous slide (inclusion of other variables, nonlinear terms, or interactions)

`r countdown::countdown(minutes = 10, top = 0.5, bottom = 0.5, right = 0.5, left = 0.75)`

---
# Evaluation on test set
* Note we wouldn't usually jump to this point yet, but let's try.


--
### First: Extract test dataset

```{r test}
test <- testing(splt)
test
```

---
# Fit model

```{r pred}
test_mod <- mod %>% 
  fit(score ~ tag_ed_fg + sp_ed_fg + econ_dsvntg +
              tst_dt + ethnic_cd, 
      data = test) #<<

```

---
# Look at model
### Just for fun

```{r look-test-mod}
summary(test_mod$fit)
```

---
# Make predictions

```{r preds}
preds <- test %>% 
  mutate(estimate = predict(test_mod$fit, newdata = .)) %>% 
  select(truth = score, estimate)
preds
```

---
# Evaluate prediction

```{r test-eval}
bind_rows(
  rmse(preds, truth, estimate),
  rsq(preds, truth, estimate)
)
```

---
class: inverse center middle

# Evaluation metrics
### Quickly

---
# Big picture
* We've spent most of today talking about the process of building a linear model. But how you determine what is "best" depends (in part) on your evaluation metric.

--

.Large[.center[For regression problems]]

--

.pull-left[
### Well known metrics

* MSE - mean square error
* RMSE - root mean square error
* $R^2$
]

--

.pull-right[
### Less well-known metrics
* MAE - mean absolute error
* RPIQ - ratio of performance to inter-quartile
* Huber loss - balances MSE and MAE
]

--

see [yardstick documentation](https://tidymodels.github.io/yardstick/reference/index.html) for complete reference of metrics implemented.

---
# A few notes
* RMSE is the mean error, but on the scale of the outcome
  + .b[Pros]: Generally fairly interpretable - how much are your predictions off by, on average?
  + .r[Cons]: The mean (generally) can be suceptible to outliers

 
--
* MAE is the median error
  + .b[Pros]: Less influenced by outliers
  + .r[Cons]: Somewhat less familiar; sometimes the outlier cases are highly important


--
* Huber Loss balances (R)MSE and MAE
  + Quadratic for small residual values, linear for large residual values
  + .b[Pros]: Sensitivity of RMSE with robustness of MAE
  + .r[Cons]: Less familiar and interpretable; does not translate to common measure of central tendancy

---
# Huber loss vs squared errors
```{r huber-loss, fig.height = 6}
kintr::include_graphics(https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Huber_loss.svg/1920px-Huber_loss.svg.png)
```

---
# Overall takeaway
* There are more metrics than described above.

--
* Choosing a metric can be really important, and often is driven by the specific problem

.g[(Example from handwriting recognition algorithm)]

---
# Kaggle
### Moving back to our model
* What do we need to do to make predictions from this model for kaggle?


--
* Preprocess the `train.csv` using the recipe we used above (particularly if you have interactions, polynomial terms, etc.)


--
* Fit the model to the preprocessed data


--
* Make predictions
  
  + Will we be able to evaluate?
  

--
* Upload a csv file that has a `Id` and `Predicted` columns. 


---
class: inverse center middle

# Next time
### Lab 2
