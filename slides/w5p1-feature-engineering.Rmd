---
title: "Feature engineering"
subtitle: "An overview of the {recipes} package and some PCA"
author: "Daniel Anderson & Joe Nese"
date: "Week 5, Class 1"
output:
  xaringan::moon_reader:
    css: ["default", "uo", "uo-fonts", "hygge", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-dune-light
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.width = 13, 
                      message = FALSE, 
                      warning = FALSE,
                      echo = TRUE)

library(tidyverse)

update_geom_defaults('path', list(size = 3, color = "cornflowerblue"))
update_geom_defaults('point', list(size = 3, color = "gray60"))
theme_set(theme_minimal(base_size = 25))
```

# Agenda 
* Basics of recipes
  - Formulas & specifying roles

* Handling categorical data

* Normalization

* Filtering

* General modifcations

* Transformations

* Missing data

* PCA

---

.center[
![](https://github.com/tidymodels/recipes/raw/master/man/figures/logo.png)
]

* Alternative package for creating a .b[design matrix] (i.e., alternative to `model.matrix`).

* More extensible than existing systems

* Has some increases in efficiency

* Ensures operations are conducted by fold

* Side benefit - really forces you (the analyst) to think each step through

---
# recipe basics

* Define a "recipe" or blueprint for feature engineering

* Iteratively apply this blueprint to each fold during training

* Carry that blueprint forward to the test data

---
# Getting started
.ital[.b[Please follow along!]]

```{r load-data}
library(tidyverse)
library(tidymodels)
d <- read_csv(here::here("data",
                         "edld-654-spring-2020",
                         "train.csv")) %>% 
  select(-classification)

splt <- initial_split(d)
train <- training(splt)
```

---
# Formula
* As we've seen, we start by applying a formula.  

```{r rmd}
rec <- recipe(score ~ ., train)
```

* Notice we use our training dataset, not the CV splits (which we actually haven't even created yet).

* The data is only used to get the column names

---
# Blueprint vs Prepped
```{r prep}
rec

prep(rec)
```

---
# A problem
* Our current formula specifies .ital[.r[everything]] that is not `score` to be a predictor. Is that reasonable?


--
.center[

![](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmedia.tenor.com%2Fimages%2F76d32a23ea4709821d1779abaa9211ab%2Ftenor.gif&f=1&nofb=1)

]

--
### Why?

* We have numerous ID variables (among other problems)

---
# Update roles

```{r update-role}
rec <- recipe(score ~ ., train) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars")
rec
```

---
# Encoding categorical data
* Most of the columns in our dataset are categorical. We can't enter them directly as predictors - they need to be dummy coded.

* The formula interface usually does this for us. {recipes} makes us declare this explicitly.

* Helper functions
  + `all_predictors()`, `all_outcomes()` `all_nominal()`, `all_numeric`

---
# Dummy code

```{r dummy1}
rec <- recipe(score ~ ., train) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_dummy(all_nominal())
rec
```

---
# View the prepped version

```{r prep-rec-dummy, error = TRUE}
prep(rec)
```

---
# Filter
* Remove zero variance predictors

```{r rm-zero-var1}
rec <- recipe(score ~ ., train) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_zv(all_predictors()) %>% 
  step_dummy(all_nominal())
rec
```

---
# Try prepped version again

```{r prep-rec-zv-dummy1, error = TRUE}
prep(rec)
```

---
# Why doesn't this work?

* I think it's a bug (and I [filed an issue](https://github.com/tidymodels/recipes/issues/486) describing this).


--
### A workaround

* Use `step_nzv` and set `freq_cut = 0` and `unique_cut = 0`. This will only eliminate constant variables.

```{r rem-zero-var2}
rec <- recipe(score ~ ., train) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% 
  step_dummy(all_nominal())
```

---
# Prepped version
### For realsies

```{r prep-rec-zv-dummy2}
prep(rec)
```

---
# Double check

```{r train-check-constants}
train %>% 
  count(lang_cd)

train %>% 
  count(calc_admn_cd)
```

---
# Apply the blueprint
* We're going to go deeper with this, but first, let's look at what this blueprint is doing.

.g[could also use `juice` in this case, but `bake` is more general]

```{r bake}
rec %>% 
  prep %>% 
  bake(train)
```

---
# A problem
* Our data variable was read in as a string. Let's fix that. 

.b[Note]: We could do this inside or out of the recipe, it doesn't really matter, but doing it as part of the recipe will make for easier transportability to the test dataset.

```{r fix-date}
rec <- recipe(score ~ ., train) %>% 
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %>% #<<
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% 
  step_dummy(all_nominal())
```

---
```{r bake-new-split}
rec %>% 
  prep %>% 
  bake(train)
```

---
# Alternatives
* You're probably most familiar with dummy coding .b[but] there there are alternatives


--
* One-hot encoding
  + Essentially equivalent to dummy-coding, but does not leave a group out (no need for a reference group in many modeling applications)
  

--
* Integer encoding
  + Assign a unique integer to each level - common in NLP applications


--
* Leave them as is
  + Tree-based methods and other applications may work just as well without any encoding
  
---
# Other considerations
* What if you have 500 rows, and a categorical variable that has 127 levels?
  + Look at the frequency of each category
  + Consider collapsing categories with small $n$ using `step_other`
  + Number of categories to retain could be treated as a hyperparameter during training

---
# Let's add some new variables
### From ODE
.g[Could get data from NCES or others too, of course]

.r[You don't need to follow along here]

```{r add-nces-data}
tmpfile <- tempfile()
download.file(
  "https://www.oregon.gov/ode/reports-and-data/students/Documents/fallmembershipreport_20192020.xlsx",
  tmpfile
)
sheets <- readxl::excel_sheets(tmpfile)
ode_schools <- readxl::read_xlsx(tmpfile,
                                 sheet = sheets[4])
ode_schools
```

---
# Pull percentage variables
```{r ode-percentages}
ethnicities <- ode_schools %>% 
  select(attnd_schl_inst_id = `Attending School ID`,
         sch_name = `School Name`,
         contains("%")) %>% 
  janitor::clean_names()
names(ethnicities) <- gsub("x2019_20_percent", "p", names(ethnicities))
ethnicities
```

---
# Join
```{r join-schl-ethnicities}
train <- left_join(train, ethnicities)
```

---
# Center scale
* It may make sense to center/scale these proportion variables
  + centering will reduce collinearity
  + scaling can help regularization methods treat variables more equivalently.
    - Note, in the below, we're also centering/scaling variables like "grade"

```{r center-scale}
rec <- recipe(score ~ ., train) %>% 
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% 
  step_center(all_numeric(), -all_outcomes(), -has_role("id vars")) %>% 
  step_scale(all_numeric(), -all_outcomes(), -has_role("id vars")) %>% 
  step_dummy(all_nominal())
```

---
# Prepped
```{r prep-center-scale}
prep(rec)
```


---
class: inverse center middle
# Missing data

---
# Missingness
* Notice we have a lot of missing data - every row of the data frame has at least one missing observation


--
* For some models, this is not a big deal - estimate on the available data (e.g., some CART models)


--
* For most, you'll need to handle it somehow:

  + Delete missing values (rows)
  + Encode missingess
  + Impute missingness
  
---
# Deletion
* Most straightforward, but the often dangerous
  + Is the missgness systematic? (leading to systematic biases in your predictions)
  
```{r rec-missingness}
rec <- recipe(score ~ ., train) %>% 
  step_naomit(all_predictors())

rec %>% 
  prep() %>% 
  bake(train)
```

---
# Encode missingness
* For categorical variables, you can .b[model] the missingness by recoding the missing values to an "unknown" category

```{r label, options}
rec <- recipe(score ~ ., train) %>% 
  step_unknown(all_nominal())

nrow(train)
rec %>% 
  prep() %>% 
  bake(train) %>% 
  select(id, ayp_lep)
```

---
# Imputation
Alternatively, you can create a model .b[for] the missingness. 

--
* Essentially equivalent to what we're doing all term long


--
* Treat the variable you are imputing as the outcome
  + Build a model with all other variables predicting the observed values
  + Use the model to predict missingness

--

.caution[Caution!]

--
* This .bolder[.b[will not]] fix MNAR issues

---
# What models?
* Very simple

  + Mean/median/mode imputation w/`step_*impute()`
  
  + Time series rolling window imputation w/`step_rollimpute`
  
    - by default provides a median imputation
  
  + Lower bound imputation w/`step_lowerimpute`
  
 
--
* More complicated

  + K-Nearest Neighbor imputation w/`step_knnimpute`
  
  + Bagged trees imputation w/`step_bagimpute`
  
---
# A few examples

```{r airquality}
head(airquality)
```


---
Rows 5/6 have been mean imputed for `Solar.R`
```{r mean-impute}
airquality_rec <- recipe(Ozone ~ ., data = airquality) %>% 
  step_meanimpute(all_predictors()) 

airquality_rec %>% 
  prep() %>% 
  bake(airquality)
```

---
Now they've been imputed using a $k$ nearest neighbor model
```{r knn-impute}
airquality_rec <- recipe(Ozone ~ ., data = airquality) %>% 
  step_knnimpute(all_predictors()) #<<

airquality_rec %>% 
  prep() %>% 
  bake(airquality)
```

---
And finally with a bagged tree model
```{r bag-impute}
airquality_rec <- recipe(Ozone ~ ., data = airquality) %>% 
  step_bagimpute(all_predictors()) #<<

airquality_rec %>% 
  prep() %>% 
  bake(airquality)
```

---
# Which works best?
* Empirical question - i.e., part of your model development process (could be considered a hyperparamter)

* Do you want to only impute for your predictors? Or outcomes too?

  + Probably depends on your model, but generally it's more important to have complete data on your predictor variables than your outcome variable(s).


--
.caution[Reminder]

* Missing data is a big topic, and even the more advanced methods .r[will not] fix MNAR data.


---
class: inverse center middle
# Transformations and other considerations

---
# Types of transformations
* Change the scale

* Expand

---
# An example

```{r seg-data}
data(segmentationData, package = "caret")
seg <- segmentationData %>% 
  filter(Case == "Train") %>% 
  select(EqSphereAreaCh1, PerimCh1, Class) %>% 
  setNames(c("PredictorA", "PredictorB", "Class")) %>% 
  mutate(Class = factor(ifelse(Class == "PS", "One", "Two"))) %>% 
  as_tibble()
seg
```

---
# Separation

```{r sep-seg-plot}
ggplot(seg, aes(PredictorA, PredictorB, color = Class)) + 
  geom_point(alpha = .5) + 
  scale_color_brewer(palette = "Accent") +
  labs(title = "Natural units")
```

---
# Inverse transformation

```{r seg-inverse}
seg %>% 
  mutate(inv_PredictorA = 1/PredictorA, 
         inv_PredictorB = 1/PredictorB) %>% 
ggplot(aes(inv_PredictorA, inv_PredictorB, color = Class)) + 
  geom_point(alpha = .5) + 
  scale_color_brewer(palette = "Accent") +
  labs(title = "Inverse scale")
```

---
# Univariate view

```{r predictora-univariate, echo = FALSE}
seg %>% 
  mutate(inv_PredictorB = 1/PredictorB) %>% 
  pivot_longer(ends_with("B"),
               names_to = "scale",
               values_to = "val") %>% 
ggplot(aes(val, fill = scale)) + 
  geom_histogram(color = "white") +
  facet_wrap(~scale, scales = "free") +
  scale_fill_brewer(palette = "Accent") +
  guides(fill = "none")
```

---
# More general transformation
### Box-Cox transformation
Originally developed as a transformation of the outcome - can help with predictor variables too.

$$
\begin{equation}
 x^* =
	\begin{cases}
  	\frac{x^\lambda-1}{\lambda}, & \text{if}\ \lambda \neq 0 \\
   	\log\left(x\right), & \text{if}\ \lambda = 0
	\end{cases}
\end{equation}
$$


--
### Objective

Estimate $\lambda$ for each  variable to be transformed

--
Technically only for positive values. Use Yeo-Johnson transformation for positive & negative data.

---
# Common $\lambda$ mappings

* $\lambda = 1$: No tranformation
* $\lambda = 0$: log tranformation
* $\lambda = 0.5$: square root tranformation
* $\lambda = -1$: inverse


--
### Box Cox transformations
```{r seg-box-cox}
bc <- recipe(Class ~ ., data = seg) %>% 
  step_BoxCox(all_predictors()) %>% 
  prep() 

tidy(bc, number = 1)
```


---
## More complicated transformations
* Nonlinear transformations may help improve performance

  + Polynomials w/`step_poly`

    - Note, these are orthogonal polynomials by default 

  + Natural- or B-spline basis functions w/`step_ns` or `step_bs`
  
    - If you're interested in splines I highly recommend [Noam Ross's free course](https://noamross.github.io/gams-in-r-course/) on GAMs.

---
background-image: url(http://www.feat.engineering/figures/numeric-ames-mars-1.svg)
background-size:cover

---
# Collapsing data w/PCA
* For some models (e.g., linear regression) highly correlated variables can reduce predictive accuracy. Collapsing variables may help.

* Basically a way to take a whole bunch of variables and reduce them down to just a few, which carry most (if not essentially all) of the same information as the raw data

* May help reduce overfitting, but if this is your primary concern, regularizatoin methods are probably better

---
# What is PCA?


---
# Downsampling
* Sometimes you have highly unbalanced data

