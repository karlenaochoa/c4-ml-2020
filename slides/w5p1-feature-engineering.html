<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Feature engineering</title>
    <meta charset="utf-8" />
    <meta name="author" content="Daniel Anderson &amp; Joe Nese" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/uo.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/uo-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/hygge.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Feature engineering
## An overview of the {recipes} package and some PCA
### Daniel Anderson &amp; Joe Nese
### Week 5, Class 1

---




# Agenda 
* Basics of recipes
  - Formulas &amp; specifying roles

* Handling categorical data

* Normalization

* Filtering

* General modifcations

* Transformations

* Missing data

* PCA

---

.center[
![](https://github.com/tidymodels/recipes/raw/master/man/figures/logo.png)
]

* Alternative package for creating a .b[design matrix] (i.e., alternative to `model.matrix`).

* More extensible than existing systems

* Has some increases in efficiency

* Ensures operations are conducted by fold

* Side benefit - really forces you (the analyst) to think each step through

---
# recipe basics

* Define a "recipe" or blueprint for feature engineering

* Iteratively apply this blueprint to each fold during training

* Carry that blueprint forward to the test data

---
# Getting started
.ital[.b[Please follow along!]]


```r
library(tidyverse)
library(tidymodels)
d &lt;- read_csv(here::here("data",
                         "edld-654-spring-2020",
                         "train.csv")) %&gt;% 
  select(-classification)

splt &lt;- initial_split(d)
train &lt;- training(splt)
```

---
# Formula
* As we've seen, we start by applying a formula.  


```r
rec &lt;- recipe(score ~ ., train)
```

* Notice we use our training dataset, not the CV splits (which we actually haven't even created yet).

* The data is only used to get the column names

---
# Blueprint vs Prepped

```r
rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         38
```

```r
prep(rec)
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         38
## 
## Training data contained 142070 data points and 142070 incomplete rows.
```

---
# A problem
* Our current formula specifies .ital[.r[everything]] that is not `score` to be a predictor. Is that reasonable?


--
.center[

![](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmedia.tenor.com%2Fimages%2F76d32a23ea4709821d1779abaa9211ab%2Ftenor.gif&amp;f=1&amp;nofb=1)

]

--
### Why?

* We have numerous ID variables (among other problems)

---
# Update roles


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  update_role(contains("id"), ncessch, new_role = "id vars")
rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    id vars          6
##    outcome          1
##  predictor         32
```

---
# Encoding categorical data
* Most of the columns in our dataset are categorical. We can't enter them directly as predictors - they need to be dummy coded.

* The formula interface usually does this for us. {recipes} makes us declare this explicitly.

* Helper functions
  + `all_predictors()`, `all_outcomes()` `all_nominal()`, `all_numeric`

---
# Dummy code


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  update_role(contains("id"), ncessch, new_role = "id vars") %&gt;% 
  step_dummy(all_nominal())
rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    id vars          6
##    outcome          1
##  predictor         32
## 
## Operations:
## 
## Dummy variables from all_nominal
```

---
# View the prepped version


```r
prep(rec)
```

```
## Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]): contrasts can be applied only to factors with 2 or more levels
```

* You can't see the full error code, but basically it's complaining because (at least) one of the variables only has one level.

---
# Filter
* Remove zero variance predictors


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  update_role(contains("id"), ncessch, new_role = "id vars") %&gt;% 
  step_zv(all_predictors()) %&gt;% 
  step_dummy(all_nominal())
rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    id vars          6
##    outcome          1
##  predictor         32
## 
## Operations:
## 
## Zero variance filter on all_predictors
## Dummy variables from all_nominal
```

---
# Try prepped version again


```r
prep(rec)
```

```
## Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]): contrasts can be applied only to factors with 2 or more levels
```


--
### Why doesn't this work?

* I think it's a bug (and I [filed an issue](https://github.com/tidymodels/recipes/issues/486) describing this).


--
### A workaround

* Use `step_nzv` and set `freq_cut = 0` and `unique_cut = 0`. This will only eliminate constant variables.


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  update_role(contains("id"), ncessch, new_role = "id vars") %&gt;% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %&gt;% 
  step_dummy(all_nominal())
```

---
# Prepped version
### For realsies


```r
prep(rec)
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    id vars          6
##    outcome          1
##  predictor         32
## 
## Training data contained 142070 data points and 142070 incomplete rows. 
## 
## Operations:
## 
## Sparse, unbalanced variable filter removed calc_admn_cd, lang_cd [trained]
## Dummy variables from gndr, ethnic_cd, tst_bnch, tst_dt, ... [trained]
```

---
# Double check


```r
train %&gt;% 
  count(lang_cd)
```

```
## # A tibble: 2 x 2
##   lang_cd      n
##   &lt;chr&gt;    &lt;int&gt;
## 1 S         3232
## 2 &lt;NA&gt;    138838
```

```r
train %&gt;% 
  count(calc_admn_cd)
```

```
## # A tibble: 1 x 2
##   calc_admn_cd      n
##   &lt;lgl&gt;         &lt;int&gt;
## 1 NA           142070
```

---
# Apply the blueprint
* We're going to go deeper with this, but first, let's look at what this blueprint is doing.

.g[could also use `juice` in this case, but `bake` is more general]


```r
rec %&gt;% 
  prep %&gt;% 
  bake(train)
```

```
## # A tibble: 142,070 x 125
##       id attnd_dist_inst_id attnd_schl_inst_id enrl_grd partic_dist_inst_id
##    &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;    &lt;dbl&gt;               &lt;dbl&gt;
##  1     1               2142               1330        6                2142
##  2     3               1995               3400        8                1995
##  3     7               1964                191        8                1964
##  4    10               1948                184        8                1948
##  5    13               1947               4396        8                1947
##  6    14               1965                201        8                1965
##  7    16               1945                168        8                1945
##  8    17               1966                208        8                1966
##  9    18               1965                201        8                1965
## 10    22               1945                168        8                1945
## # â€¦ with 142,060 more rows, and 120 more variables: partic_schl_inst_id &lt;dbl&gt;,
## #   ncessch &lt;dbl&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, score &lt;dbl&gt;, gndr_M &lt;dbl&gt;,
## #   ethnic_cd_B &lt;dbl&gt;, ethnic_cd_H &lt;dbl&gt;, ethnic_cd_I &lt;dbl&gt;, ethnic_cd_M &lt;dbl&gt;,
## #   ethnic_cd_P &lt;dbl&gt;, ethnic_cd_W &lt;dbl&gt;, tst_bnch_X2B &lt;dbl&gt;,
## #   tst_bnch_X3B &lt;dbl&gt;, tst_bnch_G4 &lt;dbl&gt;, tst_bnch_G6 &lt;dbl&gt;,
## #   tst_bnch_G7 &lt;dbl&gt;, tst_dt_X2.26.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X2.28.2018.0.00.00 &lt;dbl&gt;, tst_dt_X2.6.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X2.7.2018.0.00.00 &lt;dbl&gt;, tst_dt_X2.8.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.13.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.14.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.15.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.16.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.19.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.20.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.21.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.22.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.23.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.26.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.27.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.5.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.6.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.7.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.8.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.9.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.10.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.11.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.12.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.13.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.16.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.17.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.18.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.19.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.2.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.20.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.23.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.24.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.25.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.26.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.27.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.3.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.30.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.4.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.5.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.6.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.9.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.1.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.10.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.11.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.14.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.15.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.16.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.17.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.18.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.2.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.21.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.22.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.23.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.24.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.25.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.29.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.3.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.30.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.31.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.4.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.7.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.8.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.9.2018.0.00.00 &lt;dbl&gt;, tst_dt_X6.1.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X6.4.2018.0.00.00 &lt;dbl&gt;, tst_dt_X6.5.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X6.6.2018.0.00.00 &lt;dbl&gt;, tst_dt_X6.7.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X6.8.2018.0.00.00 &lt;dbl&gt;, migrant_ed_fg_Y &lt;dbl&gt;, ind_ed_fg_y &lt;dbl&gt;,
## #   ind_ed_fg_Y &lt;dbl&gt;, sp_ed_fg_Y &lt;dbl&gt;, tag_ed_fg_Y &lt;dbl&gt;,
## #   econ_dsvntg_Y &lt;dbl&gt;, ayp_lep_B &lt;dbl&gt;, ayp_lep_E &lt;dbl&gt;, ayp_lep_F &lt;dbl&gt;,
## #   ayp_lep_M &lt;dbl&gt;, ayp_lep_N &lt;dbl&gt;, ayp_lep_S &lt;dbl&gt;, ayp_lep_W &lt;dbl&gt;, â€¦
```

---
# A problem
* Our date variable was read in as a string. Let's fix that. 

.b[Note]: We could do this inside or outside of the recipe, it doesn't really matter, but doing it as part of the recipe will make for easier transportability to the test dataset.


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
* step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;%
  update_role(contains("id"), ncessch, new_role = "id vars") %&gt;% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %&gt;% 
  step_dummy(all_nominal())
```

---

```r
rec %&gt;% 
  prep %&gt;% 
  bake(train)
```

```
## # A tibble: 142,070 x 56
##       id attnd_dist_inst_id attnd_schl_inst_id enrl_grd tst_dt             
##    &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;    &lt;dbl&gt; &lt;dttm&gt;             
##  1     1               2142               1330        6 2018-05-14 00:00:00
##  2     3               1995               3400        8 2018-05-01 00:00:00
##  3     7               1964                191        8 2018-05-22 00:00:00
##  4    10               1948                184        8 2018-05-25 00:00:00
##  5    13               1947               4396        8 2018-05-09 00:00:00
##  6    14               1965                201        8 2018-05-11 00:00:00
##  7    16               1945                168        8 2018-05-24 00:00:00
##  8    17               1966                208        8 2018-05-10 00:00:00
##  9    18               1965                201        8 2018-05-15 00:00:00
## 10    22               1945                168        8 2018-05-23 00:00:00
## # â€¦ with 142,060 more rows, and 51 more variables: partic_dist_inst_id &lt;dbl&gt;,
## #   partic_schl_inst_id &lt;dbl&gt;, ncessch &lt;dbl&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;,
## #   score &lt;dbl&gt;, gndr_M &lt;dbl&gt;, ethnic_cd_B &lt;dbl&gt;, ethnic_cd_H &lt;dbl&gt;,
## #   ethnic_cd_I &lt;dbl&gt;, ethnic_cd_M &lt;dbl&gt;, ethnic_cd_P &lt;dbl&gt;, ethnic_cd_W &lt;dbl&gt;,
## #   tst_bnch_X2B &lt;dbl&gt;, tst_bnch_X3B &lt;dbl&gt;, tst_bnch_G4 &lt;dbl&gt;,
## #   tst_bnch_G6 &lt;dbl&gt;, tst_bnch_G7 &lt;dbl&gt;, migrant_ed_fg_Y &lt;dbl&gt;,
## #   ind_ed_fg_y &lt;dbl&gt;, ind_ed_fg_Y &lt;dbl&gt;, sp_ed_fg_Y &lt;dbl&gt;, tag_ed_fg_Y &lt;dbl&gt;,
## #   econ_dsvntg_Y &lt;dbl&gt;, ayp_lep_B &lt;dbl&gt;, ayp_lep_E &lt;dbl&gt;, ayp_lep_F &lt;dbl&gt;,
## #   ayp_lep_M &lt;dbl&gt;, ayp_lep_N &lt;dbl&gt;, ayp_lep_S &lt;dbl&gt;, ayp_lep_W &lt;dbl&gt;,
## #   ayp_lep_X &lt;dbl&gt;, ayp_lep_Y &lt;dbl&gt;, stay_in_dist_Y &lt;dbl&gt;,
## #   stay_in_schl_Y &lt;dbl&gt;, dist_sped_Y &lt;dbl&gt;, trgt_assist_fg_y &lt;dbl&gt;,
## #   trgt_assist_fg_Y &lt;dbl&gt;, ayp_dist_partic_Y &lt;dbl&gt;, ayp_schl_partic_Y &lt;dbl&gt;,
## #   ayp_dist_prfrm_Y &lt;dbl&gt;, ayp_schl_prfrm_Y &lt;dbl&gt;, rc_dist_partic_Y &lt;dbl&gt;,
## #   rc_schl_partic_Y &lt;dbl&gt;, rc_dist_prfrm_Y &lt;dbl&gt;, rc_schl_prfrm_Y &lt;dbl&gt;,
## #   tst_atmpt_fg_Y &lt;dbl&gt;, grp_rpt_dist_partic_Y &lt;dbl&gt;,
## #   grp_rpt_schl_partic_Y &lt;dbl&gt;, grp_rpt_dist_prfrm_Y &lt;dbl&gt;,
## #   grp_rpt_schl_prfrm_Y &lt;dbl&gt;
```

---
# Alternatives
* You're probably most familiar with dummy coding .b[but] there there are alternatives


--
* One-hot encoding
  + Essentially equivalent to dummy-coding, but does not leave a group out (no need for a reference group in many modeling applications)
  

--
* Integer encoding
  + Assign a unique integer to each level - common in NLP applications


--
* Leave them as is
  + Tree-based methods and other applications may work just as well without any encoding
  
---
# Other considerations
* What if you have 500 rows, and a categorical variable that has 127 levels?
  + Look at the frequency of each category
  + Consider collapsing categories with small `\(n\)` using `step_other`
  + Number of categories to retain could be treated as a hyperparameter during training

---
# Let's add some new variables
### From ODE
.g[Could get data from NCES or others too, of course]

.r[You don't need to follow along here]


```r
tmpfile &lt;- tempfile()
download.file(
  "https://www.oregon.gov/ode/reports-and-data/students/Documents/fallmembershipreport_20192020.xlsx",
  tmpfile
)
sheets &lt;- readxl::excel_sheets(tmpfile)
ode_schools &lt;- readxl::read_xlsx(tmpfile,
                                 sheet = sheets[4])
ode_schools
```

```
## # A tibble: 1,459 x 33
##    `Attending District Institution ID` `District Name` `Attending School ID`
##                                  &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt;
##  1                                2063 Adel SD 21                        498
##  2                                2113 Adrian SD 61                      707
##  3                                2113 Adrian SD 61                      708
##  4                                1899 Alsea SD 7J                        17
##  5                                2252 Amity SD 4J                      1208
##  6                                2252 Amity SD 4J                      1210
##  7                                2252 Amity SD 4J                      1209
##  8                                2252 Amity SD 4J                      4505
##  9                                2111 Annex SD 29                       705
## 10                                2111 Annex SD 29                      2111
## # â€¦ with 1,449 more rows, and 30 more variables: `School Name` &lt;chr&gt;, `2018-19
## #   Total Enrollment` &lt;chr&gt;, `2019-20 Total Enrollment` &lt;chr&gt;, `2019-20
## #   American Indian/Alaska Native` &lt;chr&gt;, `2019-20 % American Indian/Alaska
## #   Native` &lt;dbl&gt;, `2019-20 Asian` &lt;chr&gt;, `2019-20 % Asian` &lt;dbl&gt;, `2019-20
## #   Native Hawaiian/ Pacific Islander` &lt;chr&gt;, `2019-20 % Native Hawaiian/
## #   Pacific Islander` &lt;dbl&gt;, `2019-20 Black/African American` &lt;chr&gt;, `2019-20 %
## #   Black/African American` &lt;dbl&gt;, `2019-20 Hispanic/ Latino` &lt;chr&gt;, `2019-20 %
## #   Hispanic/ Latino` &lt;dbl&gt;, `2019-20 White` &lt;chr&gt;, `2019-20 % White` &lt;dbl&gt;,
## #   `2019-20 Multiracial` &lt;chr&gt;, `2019-20 % Multiracial` &lt;dbl&gt;, `2019-20
## #   Kindergarten` &lt;chr&gt;, `2019-20 Grade One` &lt;chr&gt;, `2019-20 Grade Two` &lt;chr&gt;,
## #   `2019-20 Grade Three` &lt;chr&gt;, `2019-20 Grade Four` &lt;chr&gt;, `2019-20 Grade
## #   Five` &lt;chr&gt;, `2019-20 Grade Six` &lt;chr&gt;, `2019-20 Grade Seven` &lt;chr&gt;,
## #   `2019-20 Grade Eight` &lt;chr&gt;, `2019-20 Grade Nine` &lt;chr&gt;, `2019-20 Grade
## #   Ten` &lt;chr&gt;, `2019-20 Grade Eleven` &lt;chr&gt;, `2019-20 Grade Twelve` &lt;chr&gt;
```

---
# Pull percentage variables

```r
ethnicities &lt;- ode_schools %&gt;% 
  select(attnd_schl_inst_id = `Attending School ID`,
         sch_name = `School Name`,
         contains("%")) %&gt;% 
  janitor::clean_names()
names(ethnicities) &lt;- gsub("x2019_20_percent", "p", names(ethnicities))
ethnicities
```

```
## # A tibble: 1,459 x 9
##    attnd_schl_inst_id sch_name p_american_indian_alaska_native p_asian
##                 &lt;dbl&gt; &lt;chr&gt;                              &lt;dbl&gt;   &lt;dbl&gt;
##  1                498 Adel Elâ€¦                     0.28571     0      
##  2                707 Adrian â€¦                     0           0      
##  3                708 Adrian â€¦                     0.01124     0.01124
##  4                 17 Alsea Câ€¦                     0.01558     0.05607
##  5               1208 Amity Eâ€¦                     0.02402     0      
##  6               1210 Amity Hâ€¦                     0.007940000 0.0119 
##  7               1209 Amity Mâ€¦                     0.00485     0.01456
##  8               4505 Eola Hiâ€¦                     0.0303      0      
##  9                705 Annex Câ€¦                     0           0      
## 10               2111 Annex Sâ€¦                     0           0      
## # â€¦ with 1,449 more rows, and 5 more variables:
## #   p_native_hawaiian_pacific_islander &lt;dbl&gt;, p_black_african_american &lt;dbl&gt;,
## #   p_hispanic_latino &lt;dbl&gt;, p_white &lt;dbl&gt;, p_multiracial &lt;dbl&gt;
```

---
# Join

```r
train &lt;- left_join(train, ethnicities)
```

```
## Joining, by = "attnd_schl_inst_id"
```

---
# Center scale
* It may make sense to center/scale these proportion variables
  + centering will reduce collinearity
  + scaling can help regularization methods treat variables more equivalently.
    - Note, in the below, we're also centering/scaling variables like "grade"


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;% 
  update_role(contains("id"), ncessch, new_role = "id vars") %&gt;% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %&gt;% 
* step_center(all_numeric(), -all_outcomes(), -has_role("id vars")) %&gt;%
* step_scale(all_numeric(), -all_outcomes(), -has_role("id vars")) %&gt;%
  step_dummy(all_nominal())
```

---
# Prepped

```r
prep(rec)
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    id vars          6
##    outcome          1
##  predictor         40
## 
## Training data contained 142187 data points and 142187 incomplete rows. 
## 
## Operations:
## 
## Variable mutation for tst_dt [trained]
## Sparse, unbalanced variable filter removed calc_admn_cd, lang_cd [trained]
## Centering for enrl_grd, lat, ... [trained]
## Scaling for enrl_grd, lat, ... [trained]
## Dummy variables from gndr, ethnic_cd, tst_bnch, migrant_ed_fg, ... [trained]
```


---
class: inverse center middle
# Missing data

---
# Missingness
* Notice we have a lot of missing data - every row of the data frame has at least one missing observation


--
* For some models, this is not a big deal - estimate on the available data (e.g., some CART models)


--
* For most, you'll need to handle it somehow:

  + Delete missing values (rows)
  + Encode missingess
  + Impute missingness
  
---
# Deletion
* Most straightforward, but  often dangerous
  + Is the missingness systematic? (leading to systematic biases in your predictions)
  

```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  step_naomit(all_predictors())

rec %&gt;% 
  prep() %&gt;% 
  bake(train)
```

```
## # A tibble: 0 x 47
## # â€¦ with 47 variables: id &lt;dbl&gt;, gndr &lt;fct&gt;, ethnic_cd &lt;fct&gt;,
## #   attnd_dist_inst_id &lt;dbl&gt;, attnd_schl_inst_id &lt;dbl&gt;, enrl_grd &lt;dbl&gt;,
## #   calc_admn_cd &lt;lgl&gt;, tst_bnch &lt;fct&gt;, tst_dt &lt;fct&gt;, migrant_ed_fg &lt;fct&gt;,
## #   ind_ed_fg &lt;fct&gt;, sp_ed_fg &lt;fct&gt;, tag_ed_fg &lt;fct&gt;, econ_dsvntg &lt;fct&gt;,
## #   ayp_lep &lt;fct&gt;, stay_in_dist &lt;fct&gt;, stay_in_schl &lt;fct&gt;, dist_sped &lt;fct&gt;,
## #   trgt_assist_fg &lt;fct&gt;, ayp_dist_partic &lt;fct&gt;, ayp_schl_partic &lt;fct&gt;,
## #   ayp_dist_prfrm &lt;fct&gt;, ayp_schl_prfrm &lt;fct&gt;, rc_dist_partic &lt;fct&gt;,
## #   rc_schl_partic &lt;fct&gt;, rc_dist_prfrm &lt;fct&gt;, rc_schl_prfrm &lt;fct&gt;,
## #   partic_dist_inst_id &lt;dbl&gt;, partic_schl_inst_id &lt;dbl&gt;, lang_cd &lt;fct&gt;,
## #   tst_atmpt_fg &lt;fct&gt;, grp_rpt_dist_partic &lt;fct&gt;, grp_rpt_schl_partic &lt;fct&gt;,
## #   grp_rpt_dist_prfrm &lt;fct&gt;, grp_rpt_schl_prfrm &lt;fct&gt;, ncessch &lt;dbl&gt;,
## #   lat &lt;dbl&gt;, lon &lt;dbl&gt;, sch_name &lt;fct&gt;,
## #   p_american_indian_alaska_native &lt;dbl&gt;, p_asian &lt;dbl&gt;,
## #   p_native_hawaiian_pacific_islander &lt;dbl&gt;, p_black_african_american &lt;dbl&gt;,
## #   p_hispanic_latino &lt;dbl&gt;, p_white &lt;dbl&gt;, p_multiracial &lt;dbl&gt;, score &lt;dbl&gt;
```

---
# Encode missingness
* For categorical variables, you can .b[model] the missingness by recoding the missing values to an "unknown" category


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  step_unknown(all_nominal())

rec %&gt;% 
  prep() %&gt;% 
  bake(train) %&gt;% 
  select(id, ayp_lep)
```

```
## # A tibble: 142,187 x 2
##       id ayp_lep
##    &lt;dbl&gt; &lt;fct&gt;  
##  1     1 X      
##  2     3 unknown
##  3     7 E      
##  4    10 unknown
##  5    13 unknown
##  6    14 unknown
##  7    16 unknown
##  8    17 unknown
##  9    18 unknown
## 10    22 unknown
## # â€¦ with 142,177 more rows
```

---
# Imputation
Alternatively, you can create a model .b[for] the missingness. 

--
* Essentially equivalent to what we're doing all term long


--
* Treat the variable you are imputing as the outcome
  + Build a model with all other variables predicting the observed values
  + Use the model to predict missingness

--

.caution[Caution!]

--
* This .bolder[.b[will not]] fix MNAR issues

---
# What models?
* Very simple

  + Mean/median/mode imputation w/`step_*impute()`
  
  + Time series rolling window imputation w/`step_rollimpute`
  
      - by default provides a median imputation
  
  + Lower bound imputation w/`step_lowerimpute`
  

--
* More complicated

  + K-Nearest Neighbor imputation w/`step_knnimpute`
  
  + Bagged trees imputation w/`step_bagimpute`
  
---
# A few examples


```r
head(airquality)
```

```
##   Ozone Solar.R Wind Temp Month Day
## 1    41     190  7.4   67     5   1
## 2    36     118  8.0   72     5   2
## 3    12     149 12.6   74     5   3
## 4    18     313 11.5   62     5   4
## 5    NA      NA 14.3   56     5   5
## 6    28      NA 14.9   66     5   6
```


---
Rows 5/6 have been mean imputed for `Solar.R`

```r
airquality_rec &lt;- recipe(Ozone ~ ., data = airquality) %&gt;% 
  step_meanimpute(all_predictors()) 

airquality_rec %&gt;% 
  prep() %&gt;% 
  bake(airquality)
```

```
## # A tibble: 153 x 6
##    Solar.R  Wind  Temp Month   Day Ozone
##      &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1     190   7.4    67     5     1    41
##  2     118   8      72     5     2    36
##  3     149  12.6    74     5     3    12
##  4     313  11.5    62     5     4    18
##  5     186  14.3    56     5     5    NA
##  6     186  14.9    66     5     6    28
##  7     299   8.6    65     5     7    23
##  8      99  13.8    59     5     8    19
##  9      19  20.1    61     5     9     8
## 10     194   8.6    69     5    10    NA
## # â€¦ with 143 more rows
```

---
Now they've been imputed using a `\(k\)` nearest neighbor model

```r
airquality_rec &lt;- recipe(Ozone ~ ., data = airquality) %&gt;% 
* step_knnimpute(all_predictors())

airquality_rec %&gt;% 
  prep() %&gt;% 
  bake(airquality)
```

```
## # A tibble: 153 x 6
##    Solar.R  Wind  Temp Month   Day Ozone
##      &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1     190   7.4    67     5     1    41
##  2     118   8      72     5     2    36
##  3     149  12.6    74     5     3    12
##  4     313  11.5    62     5     4    18
##  5     159  14.3    56     5     5    NA
##  6     220  14.9    66     5     6    28
##  7     299   8.6    65     5     7    23
##  8      99  13.8    59     5     8    19
##  9      19  20.1    61     5     9     8
## 10     194   8.6    69     5    10    NA
## # â€¦ with 143 more rows
```

---
And finally with a bagged tree model

```r
airquality_rec &lt;- recipe(Ozone ~ ., data = airquality) %&gt;% 
* step_bagimpute(all_predictors())

airquality_rec %&gt;% 
  prep() %&gt;% 
  bake(airquality)
```

```
## # A tibble: 153 x 6
##    Solar.R  Wind  Temp Month   Day Ozone
##      &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1     190   7.4    67     5     1    41
##  2     118   8      72     5     2    36
##  3     149  12.6    74     5     3    12
##  4     313  11.5    62     5     4    18
##  5     107  14.3    56     5     5    NA
##  6     244  14.9    66     5     6    28
##  7     299   8.6    65     5     7    23
##  8      99  13.8    59     5     8    19
##  9      19  20.1    61     5     9     8
## 10     194   8.6    69     5    10    NA
## # â€¦ with 143 more rows
```

---
# Which works best?
* Empirical question - i.e., part of your model development process (could be considered a hyperparamter)

* Do you want to only impute for your predictors? Or outcomes too?

  + Probably depends on your model, but generally it's more important to have complete data on your predictor variables than your outcome variable(s).


--
.caution[Reminder]

* Missing data is a big topic, and even the more advanced methods .r[will not] fix MNAR data.


---
class: inverse center middle
# Transformations and other considerations


---
# An example


```r
data(segmentationData, package = "caret")
seg &lt;- segmentationData %&gt;% 
  filter(Case == "Train") %&gt;% 
  select(EqSphereAreaCh1, PerimCh1, Class) %&gt;% 
  setNames(c("PredictorA", "PredictorB", "Class")) %&gt;% 
  mutate(Class = factor(ifelse(Class == "PS", "One", "Two"))) %&gt;% 
  as_tibble()
seg
```

```
## # A tibble: 1,009 x 3
##    PredictorA PredictorB Class
##         &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;
##  1  3278.726   154.8988  One  
##  2  1727.410    84.56460 Two  
##  3  1194.932   101.0911  One  
##  4  1027.222    68.71062 Two  
##  5  1035.608    73.40559 One  
##  6  1433.918    79.47569 One  
##  7   633.1043   67.36563 One  
##  8  1262.016    67.01432 Two  
##  9   985.2948   61.96803 Two  
## 10   893.0544   56.77622 Two  
## # â€¦ with 999 more rows
```

---
# Separation


```r
ggplot(seg, aes(PredictorA, PredictorB, color = Class)) + 
  geom_point(alpha = .5) + 
  scale_color_brewer(palette = "Accent") +
  labs(title = "Natural units")
```

![](w5p1-feature-engineering_files/figure-html/sep-seg-plot-1.png)&lt;!-- --&gt;

---
# Inverse transformation


```r
seg %&gt;% 
  mutate(inv_PredictorA = 1/PredictorA, 
         inv_PredictorB = 1/PredictorB) %&gt;% 
ggplot(aes(inv_PredictorA, inv_PredictorB, color = Class)) + 
  geom_point(alpha = .5) + 
  scale_color_brewer(palette = "Accent") +
  labs(title = "Inverse scale")
```

![](w5p1-feature-engineering_files/figure-html/seg-inverse-1.png)&lt;!-- --&gt;

---
# Univariate view

![](w5p1-feature-engineering_files/figure-html/predictora-univariate-1.png)&lt;!-- --&gt;

---
# More general transformation
### Box-Cox transformation
Originally developed as a transformation of the outcome - can help with predictor variables too.

$$
`\begin{equation}
 x^* =
	\begin{cases}
  	\frac{x^\lambda-1}{\lambda}, &amp; \text{if}\ \lambda \neq 0 \\
   	\log\left(x\right), &amp; \text{if}\ \lambda = 0
	\end{cases}
\end{equation}`
$$


--
### Objective

Estimate `\(\lambda\)` for each  variable to be transformed


--
Technically only for positive values. Use Yeo-Johnson transformation for positive &amp; negative data.

---
# Common `\(\lambda\)` mappings

* `\(\color{#157CAE}{\lambda} = 1\)`: No tranformation
* `\(\color{#157CAE}{\lambda} = 0\)`: log tranformation
* `\(\color{#157CAE}{\lambda} = 0.5\)`: square root tranformation
* `\(\color{#157CAE}{\lambda} = -1\)`: inverse


--
### Box Cox transformations

```r
bc &lt;- recipe(Class ~ ., data = seg) %&gt;% 
  step_BoxCox(all_predictors()) %&gt;% 
  prep() 

tidy(bc, number = 1)
```

```
## # A tibble: 2 x 3
##   terms           value id          
##   &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       
## 1 PredictorA -0.8571484 BoxCox_BFdoj
## 2 PredictorB -1.091284  BoxCox_BFdoj
```


---
## More complicated transformations
* Nonlinear transformations may help improve performance

  + Polynomials w/`step_poly`

      - Note, these are orthogonal polynomials by default 

  + Natural- or B-spline basis functions w/`step_ns` or `step_bs`
  
      - If you're interested in splines, or more generally, GAMs, I highly recommend [Noam Ross's free course](https://noamross.github.io/gams-in-r-course/) to get you started.


---
# Quick example

```r
airquality &lt;- airquality %&gt;% 
  mutate(date = lubridate::make_date(month = Month, day = Day))

ggplot(airquality, aes(date, Temp)) +
  geom_point(color = "gray70")
```

![](w5p1-feature-engineering_files/figure-html/plot-raw-1.png)&lt;!-- --&gt;


---
# Natural spline basis expansion


```r
spline_rec &lt;- recipe(Temp ~ ., airquality) %&gt;%
  step_mutate(date = as.numeric(date)) %&gt;% 
  step_ns(date) 

spline_d &lt;- spline_rec %&gt;% 
  prep() %&gt;% 
  juice()
```

---
# Fit model &amp; make prediction


```r
fit &lt;- lm(Temp ~ date_ns_1 + date_ns_2, data = spline_d)
spline_pred &lt;- spline_d %&gt;% 
  mutate(spline_pred = predict(fit, newdata = spline_d)) 

spline_pred
```

```
## # A tibble: 153 x 9
##    Ozone Solar.R  Wind Month   Day  Temp  date_ns_1    date_ns_2 spline_pred
##    &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;
##  1    41     190   7.4     5     1    67 0           0              60.74215
##  2    36     118   8       5     2    72 0.01004389 -0.006695549    61.17035
##  3    12     149  12.6     5     3    74 0.02008509 -0.01338702     61.59844
##  4    18     313  11.5     5     4    62 0.03012090 -0.02007035     62.02629
##  5    NA      NA  14.3     5     5    56 0.04014863 -0.02674146     62.45378
##  6    28      NA  14.9     5     6    66 0.05016559 -0.03339627     62.88078
##  7    23     299   8.6     5     7    65 0.06016907 -0.04003070     63.30719
##  8    19      99  13.8     5     8    59 0.07015639 -0.04664070     63.73289
##  9     8      19  20.1     5     9    61 0.08012484 -0.05322217     64.15774
## 10    NA     194   8.6     5    10    69 0.09007175 -0.05977105     64.58163
## # â€¦ with 143 more rows
```

---
# Plot predictions


```r
spline_pred %&gt;% 
  mutate(date = lubridate::make_date(month = Month, day = Day)) %&gt;% 
  ggplot(aes(date, Temp)) +
  geom_point(color = "gray70") +
  geom_line(aes(y = spline_pred),
            color = "#4f8dde")
```

![](w5p1-feature-engineering_files/figure-html/plot-preds-1.png)&lt;!-- --&gt;

---
# Finishing up on splines
* The default for `step_ns` is equivalent to `splines::ns(x, df = 2)`

* Could easily be a course on its own (and is)

* Really powerful and actually can be highly interpretable

* Can be thought of as a feature engineering consideration (as it is through recipes) rather than a model fitting procedure alone

* Splines themselves are on a predictor-by-predictor basis, but can be extended to multivariate models with GAMs

---
# Collapsing data w/PCA
* For some models (e.g., linear regression) highly correlated variables can reduce predictive accuracy. Collapsing variables may help.

* Basically a way to take a whole bunch of variables and reduce them down to just a few, which carry most of the same information as the raw data

* Can help reduce overfitting, but if this is your primary concern, regularizatoin methods are probably better

--
.bolder[.b[Goal]]: Identify a small number of dimensions (components) that account for .r[X].b[%] of the variation captured by .ital[.bolder[all]] of the variables

---
# Recipe steps to check
* Data are [tidy](https://www.jstatsoft.org/article/view/v059i10) .g[Probs fix before recipe steps]

* No missing data

* All numeric data (so need to use dummy coding, etc)

* Numeric data should be standardized (centered &amp; scaled)

---
# Get ready for PCA
Note, this is the same recipe we had before, except I've encoded and imputed missing data

```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;% 
  update_role(contains("id"), sch_name, ncessch, new_role = "id vars") %&gt;% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %&gt;% 
* step_unknown(all_nominal()) %&gt;%
* step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %&gt;%
  step_center(all_numeric(), -all_outcomes(), -has_role("id vars")) %&gt;% 
  step_scale(all_numeric(), -all_outcomes(), -has_role("id vars")) %&gt;% 
  step_dummy(all_nominal(), -has_role("id vars"))
```


---
# Retain 80% of the variance


```r
rec %&gt;% 
  step_pca(all_numeric(), -all_outcomes(), -has_role("id vars"), 
           threshold = .80) %&gt;% 
  prep() %&gt;% 
  juice() %&gt;% 
  select(id, starts_with("PC"), score)
```

```
## # A tibble: 142,187 x 7
##       id      PC1        PC2        PC3        PC4         PC5 score
##    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;
##  1     1 3.912190 -3.671239   1.448022   0.2124920  1.659494    2340
##  2     3 4.037360  2.227192  -0.5371773 -1.206262   1.791805    2551
##  3     7 3.901656  1.549484  -0.7008401 -1.075605   2.368715    2417
##  4    10 4.215254  0.6368587 -0.2505359 -1.568579   0.04774955  2620
##  5    13 4.170535  1.386503   0.3242368 -1.772688  -0.3024784   2305
##  6    14 4.218730  1.305871  -0.1019286 -1.216470   2.184753    2346
##  7    16 4.171969  0.7539188 -0.6870029 -1.599537   0.1030893   2395
##  8    17 4.187101  1.684824  -0.3270454 -1.273241   1.919880    2539
##  9    18 4.185652  1.309170  -0.1053255 -1.221911   2.176364    2392
## 10    22 4.158463  0.7469234 -0.6701712 -1.600878   0.1015258   2301
## # â€¦ with 142,177 more rows
```


---
### How many PCAs to retain 95% of the variance?


```r
rec %&gt;% 
  step_pca(all_numeric(), -all_outcomes(), -has_role("id vars"), 
           threshold = .95) %&gt;% 
  prep() %&gt;% 
  juice() %&gt;% 
  select(id, starts_with("PC"), score)
```

```
## # A tibble: 142,187 x 15
##       id     PC01       PC02       PC03       PC04        PC05       PC06
##    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;
##  1     1 3.912190 -3.671239   1.448022   0.2124920  1.659494   -0.2968687
##  2     3 4.037360  2.227192  -0.5371773 -1.206262   1.791805   -0.1387631
##  3     7 3.901656  1.549484  -0.7008401 -1.075605   2.368715    0.2807975
##  4    10 4.215254  0.6368587 -0.2505359 -1.568579   0.04774955 -0.3808101
##  5    13 4.170535  1.386503   0.3242368 -1.772688  -0.3024784  -0.6403855
##  6    14 4.218730  1.305871  -0.1019286 -1.216470   2.184753    0.6288601
##  7    16 4.171969  0.7539188 -0.6870029 -1.599537   0.1030893  -0.2148198
##  8    17 4.187101  1.684824  -0.3270454 -1.273241   1.919880    0.3426671
##  9    18 4.185652  1.309170  -0.1053255 -1.221911   2.176364    0.6230486
## 10    22 4.158463  0.7469234 -0.6701712 -1.600878   0.1015258  -0.2201829
## # â€¦ with 142,177 more rows, and 8 more variables: PC07 &lt;dbl&gt;, PC08 &lt;dbl&gt;,
## #   PC09 &lt;dbl&gt;, PC10 &lt;dbl&gt;, PC11 &lt;dbl&gt;, PC12 &lt;dbl&gt;, PC13 &lt;dbl&gt;, score &lt;dbl&gt;
```

---
class: inverse center middle
# Wrapping up

---
# Feature engineering (FE)
* Almost endless possibilities

* Probably the most "art" part of ML

* Amazing FE and a simple model will regularly beat poor FE and a fancy model

* {recipes} is a great package to do a lot of the work for you

--
Remember - it creates a .b[blueprint]! This means, we can (and should) apply the model to each fold when we're using `\(k\)`-fold CV

---
# Full recipe
### 95% of variance in PCA


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;% 
  update_role(contains("id"), sch_name, ncessch, new_role = "id vars") %&gt;% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %&gt;% 
* step_unknown(all_nominal()) %&gt;%
* step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %&gt;%
  step_center(all_numeric(), -all_outcomes(), -has_role("id vars")) %&gt;% 
  step_scale(all_numeric(), -all_outcomes(), -has_role("id vars")) %&gt;% 
  step_dummy(all_nominal(), -has_role("id vars")) %&gt;% 
  step_pca(all_numeric(), -all_outcomes(), -has_role("id vars"), 
           threshold = .95)

prepped_rec &lt;- prep(rec)
```

---
# Apply in CV
* Note the transformations are being conducted .b[for each fold], which ensures there is no data leakage


```r
cv &lt;- vfold_cv(train)
cv_baked &lt;- cv %&gt;% 
  mutate(baked = map(splits, ~bake(prepped_rec, .x)))
cv_baked
```

```
## #  10-fold cross-validation 
## # A tibble: 10 x 3
##    splits               id     baked                  
##  * &lt;named list&gt;         &lt;chr&gt;  &lt;named list&gt;           
##  1 &lt;split [128K/14.2K]&gt; Fold01 &lt;tibble [127,968 Ã— 22]&gt;
##  2 &lt;split [128K/14.2K]&gt; Fold02 &lt;tibble [127,968 Ã— 22]&gt;
##  3 &lt;split [128K/14.2K]&gt; Fold03 &lt;tibble [127,968 Ã— 22]&gt;
##  4 &lt;split [128K/14.2K]&gt; Fold04 &lt;tibble [127,968 Ã— 22]&gt;
##  5 &lt;split [128K/14.2K]&gt; Fold05 &lt;tibble [127,968 Ã— 22]&gt;
##  6 &lt;split [128K/14.2K]&gt; Fold06 &lt;tibble [127,968 Ã— 22]&gt;
##  7 &lt;split [128K/14.2K]&gt; Fold07 &lt;tibble [127,968 Ã— 22]&gt;
##  8 &lt;split [128K/14.2K]&gt; Fold08 &lt;tibble [127,969 Ã— 22]&gt;
##  9 &lt;split [128K/14.2K]&gt; Fold09 &lt;tibble [127,969 Ã— 22]&gt;
## 10 &lt;split [128K/14.2K]&gt; Fold10 &lt;tibble [127,969 Ã— 22]&gt;
```

---
class: inverse 
# Wrapping up
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atelier-dune-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
