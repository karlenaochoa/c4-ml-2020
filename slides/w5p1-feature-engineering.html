<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Feature engineering</title>
    <meta charset="utf-8" />
    <meta name="author" content="Daniel Anderson &amp; Joe Nese" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/uo.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/uo-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/hygge.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Feature engineering
## An overview of the {recipes} package and some PCA
### Daniel Anderson &amp; Joe Nese
### Week 5, Class 1

---




# Agenda 
* Basics of recipes
  - Formulas &amp; specifying roles

* Handling categorical data

* Normalization

* Filtering

* General modifcations

* Transformations

* Missing data

* PCA

---

.center[
![](https://github.com/tidymodels/recipes/raw/master/man/figures/logo.png)
]

* Alternative package for creating a .b[design matrix] (i.e., alternative to `model.matrix`).

* More extensible than existing systems

* Has some increases in efficiency

* Ensures operations are conducted by fold

* Side benefit - really forces you (the analyst) to think each step through

---
# recipe basics

* Define a "recipe" or blueprint for feature engineering

* Iteratively apply this blueprint to each fold during training

* Carry that blueprint forward to the test data

---
# Getting started
.ital[.b[Please follow along!]]


```r
library(tidyverse)
library(tidymodels)
d &lt;- read_csv(here::here("data",
                         "edld-654-spring-2020",
                         "train.csv")) %&gt;% 
  select(-classification)

splt &lt;- initial_split(d)
train &lt;- training(splt)
```

---
# Formula
* As we've seen, we start by applying a formula.  


```r
rec &lt;- recipe(score ~ ., train)
```

* Notice we use our training dataset, not the CV splits (which we actually haven't even created yet).

* The data is only used to get the column names

---
# Blueprint vs Prepped

```r
rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         38
```

```r
prep(rec)
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         38
## 
## Training data contained 142070 data points and 142070 incomplete rows.
```

---
# A problem
* Our current formula specifies .ital[.r[everything]] that is not `score` to be a predictor. Is that reasonable?


--
.center[

![](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmedia.tenor.com%2Fimages%2F76d32a23ea4709821d1779abaa9211ab%2Ftenor.gif&amp;f=1&amp;nofb=1)

]

--
### Why?

* We have numerous ID variables (among other problems)

---
# Update roles


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  update_role(contains("id"), ncessch, new_role = "id vars")
rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    id vars          6
##    outcome          1
##  predictor         32
```

---
# Encoding categorical data
* Most of the columns in our dataset are categorical. We can't enter them directly as predictors - they need to be dummy coded.

* The formula interface usually does this for us. {recipes} makes us declare this explicitly.

* Helper functions
  + `all_predictors()`, `all_outcomes()` `all_nominal()`, `all_numeric`

---
# Dummy code


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  update_role(contains("id"), ncessch, new_role = "id vars") %&gt;% 
  step_dummy(all_nominal())
rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    id vars          6
##    outcome          1
##  predictor         32
## 
## Operations:
## 
## Dummy variables from all_nominal
```

---
# View the prepped version


```r
prep(rec)
```

```
## Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]): contrasts can be applied only to factors with 2 or more levels
```

---
# Filter
* Remove zero variance predictors


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  update_role(contains("id"), ncessch, new_role = "id vars") %&gt;% 
  step_zv(all_predictors()) %&gt;% 
  step_dummy(all_nominal())
rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    id vars          6
##    outcome          1
##  predictor         32
## 
## Operations:
## 
## Zero variance filter on all_predictors
## Dummy variables from all_nominal
```

---
# Try prepped version again


```r
prep(rec)
```

```
## Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]): contrasts can be applied only to factors with 2 or more levels
```

---
# Why doesn't this work?

* I think it's a bug (and I [filed an issue](https://github.com/tidymodels/recipes/issues/486) describing this).


--
### A workaround

* Use `step_nzv` and set `freq_cut = 0` and `unique_cut = 0`. This will only eliminate constant variables.


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  update_role(contains("id"), ncessch, new_role = "id vars") %&gt;% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %&gt;% 
  step_dummy(all_nominal())
```

---
# Prepped version
### For realsies


```r
prep(rec)
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    id vars          6
##    outcome          1
##  predictor         32
## 
## Training data contained 142070 data points and 142070 incomplete rows. 
## 
## Operations:
## 
## Sparse, unbalanced variable filter removed calc_admn_cd, lang_cd [trained]
## Dummy variables from gndr, ethnic_cd, tst_bnch, tst_dt, ... [trained]
```

---
# Double check


```r
train %&gt;% 
  count(lang_cd)
```

```
## # A tibble: 2 x 2
##   lang_cd      n
##   &lt;chr&gt;    &lt;int&gt;
## 1 S         3270
## 2 &lt;NA&gt;    138800
```

```r
train %&gt;% 
  count(calc_admn_cd)
```

```
## # A tibble: 1 x 2
##   calc_admn_cd      n
##   &lt;lgl&gt;         &lt;int&gt;
## 1 NA           142070
```

---
# Apply the blueprint
* We're going to go deeper with this, but first, let's look at what this blueprint is doing.

.g[could also use `juice` in this case, but `bake` is more general]


```r
rec %&gt;% 
  prep %&gt;% 
  bake(train)
```

```
## # A tibble: 142,070 x 122
##       id attnd_dist_inst_id attnd_schl_inst_id enrl_grd partic_dist_inst_id
##    &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;    &lt;dbl&gt;               &lt;dbl&gt;
##  1     3               1995               3400        8                1995
##  2     7               1964                191        8                1964
##  3    10               1948                184        8                1948
##  4    12               1924                 84        8                1924
##  5    13               1947               4396        8                1947
##  6    17               1966                208        8                1966
##  7    18               1965                201        8                1965
##  8    21               1948                184        8                1948
##  9    22               1945                168        8                1945
## 10    24               2243               1182        8                2243
## # â€¦ with 142,060 more rows, and 117 more variables: partic_schl_inst_id &lt;dbl&gt;,
## #   ncessch &lt;dbl&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, score &lt;dbl&gt;, gndr_M &lt;dbl&gt;,
## #   ethnic_cd_B &lt;dbl&gt;, ethnic_cd_H &lt;dbl&gt;, ethnic_cd_I &lt;dbl&gt;, ethnic_cd_M &lt;dbl&gt;,
## #   ethnic_cd_P &lt;dbl&gt;, ethnic_cd_W &lt;dbl&gt;, tst_bnch_X2B &lt;dbl&gt;,
## #   tst_bnch_X3B &lt;dbl&gt;, tst_bnch_G4 &lt;dbl&gt;, tst_bnch_G6 &lt;dbl&gt;,
## #   tst_bnch_G7 &lt;dbl&gt;, tst_dt_X2.26.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X2.6.2018.0.00.00 &lt;dbl&gt;, tst_dt_X2.7.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.13.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.14.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.15.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.16.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.19.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.20.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.21.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.22.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.23.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.27.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.5.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.6.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.7.2018.0.00.00 &lt;dbl&gt;, tst_dt_X3.8.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X3.9.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.10.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.11.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.12.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.13.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.16.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.17.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.18.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.19.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.2.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.20.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.23.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.24.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.25.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.26.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.27.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.3.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.30.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.4.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.5.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X4.6.2018.0.00.00 &lt;dbl&gt;, tst_dt_X4.9.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.1.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.10.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.11.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.14.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.15.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.16.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.17.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.18.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.2.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.21.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.22.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.23.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.24.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.25.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.29.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.3.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.30.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.31.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.4.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.7.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X5.8.2018.0.00.00 &lt;dbl&gt;, tst_dt_X5.9.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X6.1.2018.0.00.00 &lt;dbl&gt;, tst_dt_X6.4.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X6.5.2018.0.00.00 &lt;dbl&gt;, tst_dt_X6.6.2018.0.00.00 &lt;dbl&gt;,
## #   tst_dt_X6.7.2018.0.00.00 &lt;dbl&gt;, tst_dt_X6.8.2018.0.00.00 &lt;dbl&gt;,
## #   migrant_ed_fg_Y &lt;dbl&gt;, ind_ed_fg_y &lt;dbl&gt;, ind_ed_fg_Y &lt;dbl&gt;,
## #   sp_ed_fg_Y &lt;dbl&gt;, tag_ed_fg_Y &lt;dbl&gt;, econ_dsvntg_Y &lt;dbl&gt;, ayp_lep_B &lt;dbl&gt;,
## #   ayp_lep_E &lt;dbl&gt;, ayp_lep_F &lt;dbl&gt;, ayp_lep_M &lt;dbl&gt;, ayp_lep_N &lt;dbl&gt;,
## #   ayp_lep_S &lt;dbl&gt;, ayp_lep_W &lt;dbl&gt;, ayp_lep_X &lt;dbl&gt;, ayp_lep_Y &lt;dbl&gt;,
## #   stay_in_dist_Y &lt;dbl&gt;, â€¦
```

---
# A problem
* Our data variable was read in as a string. Let's fix that. 

.b[Note]: We could do this inside or out of the recipe, it doesn't really matter, but doing it as part of the recipe will make for easier transportability to the test dataset.


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
* step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;%
  update_role(contains("id"), ncessch, new_role = "id vars") %&gt;% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %&gt;% 
  step_dummy(all_nominal())
```

---

```r
rec %&gt;% 
  prep %&gt;% 
  bake(train)
```

```
## # A tibble: 142,070 x 56
##       id attnd_dist_inst_id attnd_schl_inst_id enrl_grd tst_dt             
##    &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;    &lt;dbl&gt; &lt;dttm&gt;             
##  1     3               1995               3400        8 2018-05-01 00:00:00
##  2     7               1964                191        8 2018-05-22 00:00:00
##  3    10               1948                184        8 2018-05-25 00:00:00
##  4    12               1924                 84        8 2018-05-10 00:00:00
##  5    13               1947               4396        8 2018-05-09 00:00:00
##  6    17               1966                208        8 2018-05-10 00:00:00
##  7    18               1965                201        8 2018-05-15 00:00:00
##  8    21               1948                184        8 2018-05-22 00:00:00
##  9    22               1945                168        8 2018-05-23 00:00:00
## 10    24               2243               1182        8 2018-05-23 00:00:00
## # â€¦ with 142,060 more rows, and 51 more variables: partic_dist_inst_id &lt;dbl&gt;,
## #   partic_schl_inst_id &lt;dbl&gt;, ncessch &lt;dbl&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;,
## #   score &lt;dbl&gt;, gndr_M &lt;dbl&gt;, ethnic_cd_B &lt;dbl&gt;, ethnic_cd_H &lt;dbl&gt;,
## #   ethnic_cd_I &lt;dbl&gt;, ethnic_cd_M &lt;dbl&gt;, ethnic_cd_P &lt;dbl&gt;, ethnic_cd_W &lt;dbl&gt;,
## #   tst_bnch_X2B &lt;dbl&gt;, tst_bnch_X3B &lt;dbl&gt;, tst_bnch_G4 &lt;dbl&gt;,
## #   tst_bnch_G6 &lt;dbl&gt;, tst_bnch_G7 &lt;dbl&gt;, migrant_ed_fg_Y &lt;dbl&gt;,
## #   ind_ed_fg_y &lt;dbl&gt;, ind_ed_fg_Y &lt;dbl&gt;, sp_ed_fg_Y &lt;dbl&gt;, tag_ed_fg_Y &lt;dbl&gt;,
## #   econ_dsvntg_Y &lt;dbl&gt;, ayp_lep_B &lt;dbl&gt;, ayp_lep_E &lt;dbl&gt;, ayp_lep_F &lt;dbl&gt;,
## #   ayp_lep_M &lt;dbl&gt;, ayp_lep_N &lt;dbl&gt;, ayp_lep_S &lt;dbl&gt;, ayp_lep_W &lt;dbl&gt;,
## #   ayp_lep_X &lt;dbl&gt;, ayp_lep_Y &lt;dbl&gt;, stay_in_dist_Y &lt;dbl&gt;,
## #   stay_in_schl_Y &lt;dbl&gt;, dist_sped_Y &lt;dbl&gt;, trgt_assist_fg_y &lt;dbl&gt;,
## #   trgt_assist_fg_Y &lt;dbl&gt;, ayp_dist_partic_Y &lt;dbl&gt;, ayp_schl_partic_Y &lt;dbl&gt;,
## #   ayp_dist_prfrm_Y &lt;dbl&gt;, ayp_schl_prfrm_Y &lt;dbl&gt;, rc_dist_partic_Y &lt;dbl&gt;,
## #   rc_schl_partic_Y &lt;dbl&gt;, rc_dist_prfrm_Y &lt;dbl&gt;, rc_schl_prfrm_Y &lt;dbl&gt;,
## #   tst_atmpt_fg_Y &lt;dbl&gt;, grp_rpt_dist_partic_Y &lt;dbl&gt;,
## #   grp_rpt_schl_partic_Y &lt;dbl&gt;, grp_rpt_dist_prfrm_Y &lt;dbl&gt;,
## #   grp_rpt_schl_prfrm_Y &lt;dbl&gt;
```

---
# Alternatives
* You're probably most familiar with dummy coding .b[but] there there are alternatives


--
* One-hot encoding
  + Essentially equivalent to dummy-coding, but does not leave a group out (no need for a reference group in many modeling applications)
  

--
* Integer encoding
  + Assign a unique integer to each level - common in NLP applications


--
* Leave them as is
  + Tree-based methods and other applications may work just as well without any encoding
  
---
# Other considerations
* What if you have 500 rows, and a categorical variable that has 127 levels?
  + Look at the frequency of each category
  + Consider collapsing categories with small `\(n\)` using `step_other`
  + Number of categories to retain could be treated as a hyperparameter during training

---
# Let's add some new variables
### From ODE
.g[Could get data from NCES or others too, of course]

.r[You don't need to follow along here]


```r
tmpfile &lt;- tempfile()
download.file(
  "https://www.oregon.gov/ode/reports-and-data/students/Documents/fallmembershipreport_20192020.xlsx",
  tmpfile
)
sheets &lt;- readxl::excel_sheets(tmpfile)
ode_schools &lt;- readxl::read_xlsx(tmpfile,
                                 sheet = sheets[4])
ode_schools
```

```
## # A tibble: 1,459 x 33
##    `Attending District Institution ID` `District Name` `Attending School ID`
##                                  &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt;
##  1                                2063 Adel SD 21                        498
##  2                                2113 Adrian SD 61                      707
##  3                                2113 Adrian SD 61                      708
##  4                                1899 Alsea SD 7J                        17
##  5                                2252 Amity SD 4J                      1208
##  6                                2252 Amity SD 4J                      1210
##  7                                2252 Amity SD 4J                      1209
##  8                                2252 Amity SD 4J                      4505
##  9                                2111 Annex SD 29                       705
## 10                                2111 Annex SD 29                      2111
## # â€¦ with 1,449 more rows, and 30 more variables: `School Name` &lt;chr&gt;, `2018-19
## #   Total Enrollment` &lt;chr&gt;, `2019-20 Total Enrollment` &lt;chr&gt;, `2019-20
## #   American Indian/Alaska Native` &lt;chr&gt;, `2019-20 % American Indian/Alaska
## #   Native` &lt;dbl&gt;, `2019-20 Asian` &lt;chr&gt;, `2019-20 % Asian` &lt;dbl&gt;, `2019-20
## #   Native Hawaiian/ Pacific Islander` &lt;chr&gt;, `2019-20 % Native Hawaiian/
## #   Pacific Islander` &lt;dbl&gt;, `2019-20 Black/African American` &lt;chr&gt;, `2019-20 %
## #   Black/African American` &lt;dbl&gt;, `2019-20 Hispanic/ Latino` &lt;chr&gt;, `2019-20 %
## #   Hispanic/ Latino` &lt;dbl&gt;, `2019-20 White` &lt;chr&gt;, `2019-20 % White` &lt;dbl&gt;,
## #   `2019-20 Multiracial` &lt;chr&gt;, `2019-20 % Multiracial` &lt;dbl&gt;, `2019-20
## #   Kindergarten` &lt;chr&gt;, `2019-20 Grade One` &lt;chr&gt;, `2019-20 Grade Two` &lt;chr&gt;,
## #   `2019-20 Grade Three` &lt;chr&gt;, `2019-20 Grade Four` &lt;chr&gt;, `2019-20 Grade
## #   Five` &lt;chr&gt;, `2019-20 Grade Six` &lt;chr&gt;, `2019-20 Grade Seven` &lt;chr&gt;,
## #   `2019-20 Grade Eight` &lt;chr&gt;, `2019-20 Grade Nine` &lt;chr&gt;, `2019-20 Grade
## #   Ten` &lt;chr&gt;, `2019-20 Grade Eleven` &lt;chr&gt;, `2019-20 Grade Twelve` &lt;chr&gt;
```

---
# Pull percentage variables

```r
ethnicities &lt;- ode_schools %&gt;% 
  select(attnd_schl_inst_id = `Attending School ID`,
         sch_name = `School Name`,
         contains("%")) %&gt;% 
  janitor::clean_names()
names(ethnicities) &lt;- gsub("x2019_20_percent", "p", names(ethnicities))
ethnicities
```

```
## # A tibble: 1,459 x 9
##    attnd_schl_inst_id sch_name p_american_indian_alaska_native p_asian
##                 &lt;dbl&gt; &lt;chr&gt;                              &lt;dbl&gt;   &lt;dbl&gt;
##  1                498 Adel Elâ€¦                     0.28571     0      
##  2                707 Adrian â€¦                     0           0      
##  3                708 Adrian â€¦                     0.01124     0.01124
##  4                 17 Alsea Câ€¦                     0.01558     0.05607
##  5               1208 Amity Eâ€¦                     0.02402     0      
##  6               1210 Amity Hâ€¦                     0.007940000 0.0119 
##  7               1209 Amity Mâ€¦                     0.00485     0.01456
##  8               4505 Eola Hiâ€¦                     0.0303      0      
##  9                705 Annex Câ€¦                     0           0      
## 10               2111 Annex Sâ€¦                     0           0      
## # â€¦ with 1,449 more rows, and 5 more variables:
## #   p_native_hawaiian_pacific_islander &lt;dbl&gt;, p_black_african_american &lt;dbl&gt;,
## #   p_hispanic_latino &lt;dbl&gt;, p_white &lt;dbl&gt;, p_multiracial &lt;dbl&gt;
```

---
# Join

```r
train &lt;- left_join(train, ethnicities)
```

---
# Center scale
* It may make sense to center/scale these proportion variables
  + centering will reduce collinearity
  + scaling can help regularization methods treat variables more equivalently.
    - Note, in the below, we're also centering/scaling variables like "grade"


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;% 
  update_role(contains("id"), ncessch, new_role = "id vars") %&gt;% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %&gt;% 
  step_center(all_numeric(), -all_outcomes(), -has_role("id vars")) %&gt;% 
  step_scale(all_numeric(), -all_outcomes(), -has_role("id vars")) %&gt;% 
  step_dummy(all_nominal())
```

---
# Prepped

```r
prep(rec)
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    id vars          6
##    outcome          1
##  predictor         40
## 
## Training data contained 142173 data points and 142173 incomplete rows. 
## 
## Operations:
## 
## Variable mutation for tst_dt [trained]
## Sparse, unbalanced variable filter removed calc_admn_cd, lang_cd [trained]
## Centering for enrl_grd, lat, ... [trained]
## Scaling for enrl_grd, lat, ... [trained]
## Dummy variables from gndr, ethnic_cd, tst_bnch, migrant_ed_fg, ... [trained]
```


---
class: inverse center middle
# Missing data

---
# Missingness
* Notice we have a lot of missing data - every row of the data frame has at least one missing observation


--
* For some models, this is not a big deal - estimate on the available data (e.g., some CART models)


--
* For most, you'll need to handle it somehow:

  + Delete missing values (rows)
  + Encode missingess
  + Impute missingness
  
---
# Deletion
* Most straightforward, but the often dangerous
  + Is the missgness systematic? (leading to systematic biases in your predictions)
  

```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  step_naomit(all_predictors())

rec %&gt;% 
  prep() %&gt;% 
  bake(train)
```

```
## # A tibble: 0 x 47
## # â€¦ with 47 variables: id &lt;dbl&gt;, gndr &lt;fct&gt;, ethnic_cd &lt;fct&gt;,
## #   attnd_dist_inst_id &lt;dbl&gt;, attnd_schl_inst_id &lt;dbl&gt;, enrl_grd &lt;dbl&gt;,
## #   calc_admn_cd &lt;lgl&gt;, tst_bnch &lt;fct&gt;, tst_dt &lt;fct&gt;, migrant_ed_fg &lt;fct&gt;,
## #   ind_ed_fg &lt;fct&gt;, sp_ed_fg &lt;fct&gt;, tag_ed_fg &lt;fct&gt;, econ_dsvntg &lt;fct&gt;,
## #   ayp_lep &lt;fct&gt;, stay_in_dist &lt;fct&gt;, stay_in_schl &lt;fct&gt;, dist_sped &lt;fct&gt;,
## #   trgt_assist_fg &lt;fct&gt;, ayp_dist_partic &lt;fct&gt;, ayp_schl_partic &lt;fct&gt;,
## #   ayp_dist_prfrm &lt;fct&gt;, ayp_schl_prfrm &lt;fct&gt;, rc_dist_partic &lt;fct&gt;,
## #   rc_schl_partic &lt;fct&gt;, rc_dist_prfrm &lt;fct&gt;, rc_schl_prfrm &lt;fct&gt;,
## #   partic_dist_inst_id &lt;dbl&gt;, partic_schl_inst_id &lt;dbl&gt;, lang_cd &lt;fct&gt;,
## #   tst_atmpt_fg &lt;fct&gt;, grp_rpt_dist_partic &lt;fct&gt;, grp_rpt_schl_partic &lt;fct&gt;,
## #   grp_rpt_dist_prfrm &lt;fct&gt;, grp_rpt_schl_prfrm &lt;fct&gt;, ncessch &lt;dbl&gt;,
## #   lat &lt;dbl&gt;, lon &lt;dbl&gt;, sch_name &lt;fct&gt;,
## #   p_american_indian_alaska_native &lt;dbl&gt;, p_asian &lt;dbl&gt;,
## #   p_native_hawaiian_pacific_islander &lt;dbl&gt;, p_black_african_american &lt;dbl&gt;,
## #   p_hispanic_latino &lt;dbl&gt;, p_white &lt;dbl&gt;, p_multiracial &lt;dbl&gt;, score &lt;dbl&gt;
```

---
# Encode missingness
* For categorical variables, you can .b[model] the missingness by recoding the missing values to an "unknown" category


```r
rec &lt;- recipe(score ~ ., train) %&gt;% 
  step_unknown(all_nominal())

nrow(train)
```

```
## [1] 142173
```

```r
rec %&gt;% 
  prep() %&gt;% 
  bake(train) %&gt;% 
  select(id, ayp_lep)
```

```
## # A tibble: 142,173 x 2
##       id ayp_lep
##    &lt;dbl&gt; &lt;fct&gt;  
##  1     3 unknown
##  2     7 E      
##  3    10 unknown
##  4    12 N      
##  5    13 unknown
##  6    17 unknown
##  7    18 unknown
##  8    21 unknown
##  9    22 unknown
## 10    24 unknown
## # â€¦ with 142,163 more rows
```

---
# Imputation
Alternatively, you can create a model .b[for] the missingness. 

--
* Essentially equivalent to what we're doing all term long


--
* Treat the variable you are imputing as the outcome
  + Build a model with all other variables predicting the observed values
  + Use the model to predict missingness

--

.caution[Caution!]

--
* This .bolder[.b[will not]] fix MNAR issues

---
# What models?
* Very simple

  + Mean/median/mode imputation w/`step_*impute()`
  
  + Time series rolling window imputation w/`step_rollimpute`
  
    - by default provides a median imputation
  
  + Lower bound imputation w/`step_lowerimpute`
  
 
--
* More complicated

  + K-Nearest Neighbor imputation w/`step_knnimpute`
  
  + Bagged trees imputation w/`step_bagimpute`
  
---
# A few examples


```r
head(airquality)
```

```
##   Ozone Solar.R Wind Temp Month Day
## 1    41     190  7.4   67     5   1
## 2    36     118  8.0   72     5   2
## 3    12     149 12.6   74     5   3
## 4    18     313 11.5   62     5   4
## 5    NA      NA 14.3   56     5   5
## 6    28      NA 14.9   66     5   6
```


---
Rows 5/6 have been mean imputed for `Solar.R`

```r
airquality_rec &lt;- recipe(Ozone ~ ., data = airquality) %&gt;% 
  step_meanimpute(all_predictors()) 

airquality_rec %&gt;% 
  prep() %&gt;% 
  bake(airquality)
```

```
## # A tibble: 153 x 6
##    Solar.R  Wind  Temp Month   Day Ozone
##      &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1     190   7.4    67     5     1    41
##  2     118   8      72     5     2    36
##  3     149  12.6    74     5     3    12
##  4     313  11.5    62     5     4    18
##  5     186  14.3    56     5     5    NA
##  6     186  14.9    66     5     6    28
##  7     299   8.6    65     5     7    23
##  8      99  13.8    59     5     8    19
##  9      19  20.1    61     5     9     8
## 10     194   8.6    69     5    10    NA
## # â€¦ with 143 more rows
```

---
Now they've been imputed using a `\(k\)` nearest neighbor model

```r
airquality_rec &lt;- recipe(Ozone ~ ., data = airquality) %&gt;% 
* step_knnimpute(all_predictors())

airquality_rec %&gt;% 
  prep() %&gt;% 
  bake(airquality)
```

```
## # A tibble: 153 x 6
##    Solar.R  Wind  Temp Month   Day Ozone
##      &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1     190   7.4    67     5     1    41
##  2     118   8      72     5     2    36
##  3     149  12.6    74     5     3    12
##  4     313  11.5    62     5     4    18
##  5     159  14.3    56     5     5    NA
##  6     220  14.9    66     5     6    28
##  7     299   8.6    65     5     7    23
##  8      99  13.8    59     5     8    19
##  9      19  20.1    61     5     9     8
## 10     194   8.6    69     5    10    NA
## # â€¦ with 143 more rows
```

---
And finally with a bagged tree model

```r
airquality_rec &lt;- recipe(Ozone ~ ., data = airquality) %&gt;% 
* step_bagimpute(all_predictors())

airquality_rec %&gt;% 
  prep() %&gt;% 
  bake(airquality)
```

```
## # A tibble: 153 x 6
##    Solar.R  Wind  Temp Month   Day Ozone
##      &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1     190   7.4    67     5     1    41
##  2     118   8      72     5     2    36
##  3     149  12.6    74     5     3    12
##  4     313  11.5    62     5     4    18
##  5     117  14.3    56     5     5    NA
##  6     237  14.9    66     5     6    28
##  7     299   8.6    65     5     7    23
##  8      99  13.8    59     5     8    19
##  9      19  20.1    61     5     9     8
## 10     194   8.6    69     5    10    NA
## # â€¦ with 143 more rows
```

---
# Which works best?
* Empirical question - i.e., part of your model development process (could be considered a hyperparamter)

* Do you want to only impute for your predictors? Or outcomes too?

  + Probably depends on your model, but generally it's more important to have complete data on your predictor variables than your outcome variable(s).


--
.caution[Reminder]

* Missing data is a big topic, and even the more advanced methods .r[will not] fix MNAR data.


---
class: inverse center middle
# Transformations and other considerations

---
# Types of transformations
* Change the scale

* Expand

---
# An example


```r
data(segmentationData, package = "caret")
seg &lt;- segmentationData %&gt;% 
  filter(Case == "Train") %&gt;% 
  select(EqSphereAreaCh1, PerimCh1, Class) %&gt;% 
  setNames(c("PredictorA", "PredictorB", "Class")) %&gt;% 
  mutate(Class = factor(ifelse(Class == "PS", "One", "Two"))) %&gt;% 
  as_tibble()
seg
```

```
## # A tibble: 1,009 x 3
##    PredictorA PredictorB Class
##         &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;
##  1  3278.726   154.8988  One  
##  2  1727.410    84.56460 Two  
##  3  1194.932   101.0911  One  
##  4  1027.222    68.71062 Two  
##  5  1035.608    73.40559 One  
##  6  1433.918    79.47569 One  
##  7   633.1043   67.36563 One  
##  8  1262.016    67.01432 Two  
##  9   985.2948   61.96803 Two  
## 10   893.0544   56.77622 Two  
## # â€¦ with 999 more rows
```

---
# Separation


```r
ggplot(seg, aes(PredictorA, PredictorB, color = Class)) + 
  geom_point(alpha = .5) + 
  scale_color_brewer(palette = "Accent") +
  labs(title = "Natural units")
```

![](w5p1-feature-engineering_files/figure-html/sep-seg-plot-1.png)&lt;!-- --&gt;

---
# Inverse transformation


```r
seg %&gt;% 
  mutate(inv_PredictorA = 1/PredictorA, 
         inv_PredictorB = 1/PredictorB) %&gt;% 
ggplot(aes(inv_PredictorA, inv_PredictorB, color = Class)) + 
  geom_point(alpha = .5) + 
  scale_color_brewer(palette = "Accent") +
  labs(title = "Inverse scale")
```

![](w5p1-feature-engineering_files/figure-html/seg-inverse-1.png)&lt;!-- --&gt;

---
# Univariate view

![](w5p1-feature-engineering_files/figure-html/predictora-univariate-1.png)&lt;!-- --&gt;

---
# More general transformation
### Box-Cox transformation
Originally developed as a transformation of the outcome - can help with predictor variables too.

$$
`\begin{equation}
 x^* =
	\begin{cases}
  	\frac{x^\lambda-1}{\lambda}, &amp; \text{if}\ \lambda \neq 0 \\
   	\log\left(x\right), &amp; \text{if}\ \lambda = 0
	\end{cases}
\end{equation}`
$$


--
### Objective

Estimate `\(\lambda\)` for each  variable to be transformed


---
# Common `\(\lambda\)` mappings

* `\(\lambda = 1\)`: No tranformation
* `\(\lambda = 0\)`: log tranformation
* `\(\lambda = 0.5\)`: square root tranformation
* `\(\lambda = -1\)`: inverse


--
### Box Cox transformations

```r
bc &lt;- recipe(Class ~ ., data = seg) %&gt;% 
  step_BoxCox(all_predictors()) %&gt;% 
  prep() 

tidy(bc, number = 1)
```

```
## # A tibble: 2 x 3
##   terms           value id          
##   &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       
## 1 PredictorA -0.8571484 BoxCox_R1dOc
## 2 PredictorB -1.091284  BoxCox_R1dOc
```

---
## More complicated transformations
* Nonlinear transformations may help improve performance
  + polynomials
  + splines
  + hinges

---
background-image: url(http://www.feat.engineering/figures/numeric-ames-mars-1.svg)
background-size:cover

---
# Collapsing data w/PCA
* For some models (e.g., linear regression) highly correlated variables can reduce predictive accuracy. Collapsing variables may help.

* Basically a way to take a whole bunch of variables and reduce them down to just a few, which carry most (if not essentially all) of the same information as the raw data

* May help reduce overfitting, but if this is your primary concern, regularizatoin methods are probably better

---
# What is PCA?


---
# Downsampling
* Sometimes you have highly unbalanced data
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atelier-dune-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
